<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[InnoDB中一条update语句到底涉及到了哪些内容呢？]]></title>
    <url>%2F2020%2F12%2F17%2F1%2F</url>
    <content type="text"><![CDATA[InnoDB在处理更新语句的时候，并不会将数据直接更新到数据库，大量的磁盘操作势必影响数据库的性能，InnoDB是怎么做的呢？InnoDB通过引入内存组件buffer pool从而避免频繁对磁盘做随机读写操作，并引入redo log(写入日志)的机制保证了数据不丢失。采用缓存必然存在缓存不足需要进行内存淘汰,InnoDB则采取一种类似lru的内存淘汰算法。 本文将以InnoDB作为存储引擎的Mysql展开论述。 update语句执行过程如何上锁？update语句在执行的时候则是当前读，得到最新的信息并且锁定相应的记录。一条update语句在执行过程中的锁定规则其实是依据InnoDB行锁规则。 InnoDB行锁是通过给索引上的索引项加锁来实现的，InnoDB这种行锁实现特点意味着：只有通过索引条件检索数据，InnoDB才使用行级锁，否则，InnoDB将使用表锁。 因此更新条件为非索引字段，行锁会升级成表锁。更新条件为索引字段时，行锁会锁定满足条件的行。 为什么写入缓存却可以保证数据不丢失？数据写入缓存并不能保证数据不丢失，因此InnoDB引入了 redo log(重做日志)。 WALWAL的全称是Write-Ahead Logging，Mysql在更新数据时采用了一种日志先行的策略,先写日志，再写磁盘。 当有一条记录需要更新的时候，InnoDB引擎就会先把记录写到redo log里面，并更新内存,然后会在适当的时候，将这个操作记录更新到磁盘。 redo log记录的是相对mysql磁盘中的增量数据，可以保证即使数据库发生异常重启，内存中的数据清空了，之前提交的记录页不会丢失，具备crash-safe的能力。 buffer pool 一探究竟内存的数据页是在Buffer Pool 中管理的，通常以页(page)为单位缓存数据，每次读写数据都会通过 Buffer Pool。在WAL里Buffer Pool 起到了加速更新的作用。同时查询数据时，当Buffer Pool 中存在用户所需要的数据时，直接返回。没有时才去硬盘中获取并更新Buffer Pool，也保证了查询的效率。 基本概念数据在内存中更改后并不会立刻写入到磁盘中，这些数据被叫做脏数据。 当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为脏页。 内存数据写入到磁盘后，内存和磁盘上的数据页的内容一致，称为干净页。 把内存里的数据写入磁盘的过程被称之为Flush。 通过 innodb_buffer_pool_size设置缓存的总容量。 mysql5.6 引入 innodb_buffer_pool_instances 可以设置多个缓冲池对象，此参数默认为8，主要目的是为了解决 互斥锁， 每个缓冲池管理其自己的空闲列表，提高查询并发性。 脏页刷新规则mysql刷脏页会在以下四中场景下触发： 内存不够用: 如果内存不够用了会根据LRU算法淘汰旧数据，如果淘汰的是脏页必须先刷脏。 redo log写满: redo log写满了，mysql必须停止所有的更新操作，然后把checkpoint往前推进，那么推进的这一部分对应的脏页都需要刷到磁盘上。 mysql正常关闭: mysql正常关闭时后，会将内存中的脏页全部刷到磁盘上。 mysql空闲时: mysql在服务器负载较小时会主动刷脏页。 当内存不足时，淘汰的数据页为脏页时，进行刷脏。redo log重放时，如果一个数据页已经是刷过，会识别出来并跳过。 InnoDB刷脏页的控制策略通过设置innodb_io_capacity告知InnoDB你的磁盘能力,InnoDB会根据磁盘能力计算出刷盘的速率。建议设置成磁盘的IOPS。磁盘的 IOPS可以通过fio这个工具来测试 innodb_io_capacity设置过小会导致刷太慢，造成脏页累积，其次是redo log写满。所以，InnoDB的刷盘速度就是要参考这两个因素:一个是脏页比例，一个是redo log写盘速度。 参数：innodb_max_dirty_pages_pct 是脏页比例上限，默认值是75%。 脏页比例是通过Innodb_buffer_pool_pages_dirty/Innodb_buffer_pool_pages_total得到的 连坐策略MySQL中，刷脏页时，如果这个数据页旁边的数据页刚好是脏页，就会把旁边的脏页一并刷掉，而且这个逻辑还可以继续蔓延。 InnoDB中的innodb_flush_neighbors 参数就是用来控制这个行为的，值为1的时候会有上述的连坐机制，值为0时表示不会蔓延。 这种机制的优化在机械硬盘时代是很有意义的，可以减少很多随机IO。当使用的是SSD这类IOPS比较高的设备的话，随机IO并非瓶颈，建议设置成0。 内存淘汰策略而Buffer Pool的存在对查询起到加速效果，但如果多次查询都没有命中，内存命中率不高其实就没有太大意义了，通过show engine innodb status可以查看BP命中率。InnoDB 的Buffer Pool的大小是由参数 innodb_buffer_pool_size确定的，一般建议设置成可用物理内存的60%~80%。Buffer Pool满了，而又要从磁盘读入一个数据页，那肯定是要淘汰一个旧数据页的。一个好的淘汰策略会极大的影响之后查询的内存命中率。 InnoDB内存管理使用的是LRU算法，但在此基础上做了一些改进。 为什么不能直接使用LRU算法LRU算法一般会使用链表和hash来实现。新的数据会写入到链表的头部，当遭遇内存不足时，会将尾部数据进行淘汰。 在mysql中，该算法看似可以满足。但是mysql中出现全表扫描时，表数据特别大，便会把当前的Buffer Pool里的数据全部淘汰掉，存入扫描过程中访问到的数据页的内容。但该表数据在平时业务场景下并不会访问，加载到Buffer Pool中的页并没有被使用，最终便会导致Buffer Pool的内存命中率急剧下降，磁盘压力增加，SQL语句响应变慢。 分层的LRU算法正是存在大量的使用频率偏低的页被同时加载到Buffer Pool时，可能会把那些使用频率非常高的页从Buffer Pool中淘汰掉的情况。Mysql按照5:3(innodb_old_blocks_pct参数控制)的比例把整个LRU链表分成了young区域和old区域。 改进后的LRU算法执行流： 磁盘中页数据第一次加载到内存中时，先缓存到链表old区的头部，这样预读的数据没使用的话，就会逐渐被淘汰掉；且不会影响到young区域中使用频繁的缓存数据。 处于old区域的数据页，第一次访问时就在它对应的控制块中记录下来这个访问时间，如果后续的访问时间与第一次访问的时间间隔在1秒(时间由参数 innodb_old_blocks_time 控制，默认值是1000，单位毫秒)以内，那么该页面就不会被从old区域移动到young区域的头部，否则将它移动到young区域的头部。 为什么需要引入innodb_old_blocks_time的机制呢？ 一个数据页里面有多条记录被访问，这个数据页也会被多次访问，当数据库在处理全表扫描时，一个数据页里面有多条记录，这个数据页会被多次访问，但由于是顺序扫描，这个数据页第一次被访问和最后一次被访问的时间间隔会很短。因此还是保证这次查询时数据页仍就被保留在old区域。 通过优化后的LRU算法在扫描大表的过程中，虽然也用到了Buffer Pool，但是对young区域完全没有影响，从而保证Buffer Pool的缓存命中率。 参考： mysql 45讲 - 02讲日志系统：一条SQL更新语句是如何执行的 mysql 45讲 - 12讲为什么我的MySQL会“抖”一下 mysql 45讲 - 33讲我查这么多数据，会不会把数据库内存打爆 从一个 update 语句开始，来看看 InnoDB 的底层架构原理]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>lock</tag>
        <tag>lru</tag>
        <tag>buffer pool</tag>
        <tag>redo log</tag>
        <tag>wal</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql-lock]]></title>
    <url>%2F2020%2F11%2F08%2F1%2F</url>
    <content type="text"><![CDATA[锁是计算机协调多个进程或线程并发访问某一资源的机制。而根据加锁的范围，可被分为全局锁、表级锁和行锁。他们都有什么作用呢？有让我们一探究竟吧！ 全局锁FTWRL全局锁将对整个数据库实例加锁，使得整个库处于只读状态，会阻塞DDL和部分DML(select不被阻塞)语句。通过输入命令 Flush tables with read lock (FTWRL)让整个库处于只读状态。 全局锁的典型使用场景是做全库逻辑备份。但是在备份过程中整个库完全处于只读状态，因此会出现一些隐患： 如果在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆； 如果你在从库上备份，那么备份期间从库不能执行主库同步过来的binlog，会导致主从延迟。 如此大的隐患势必在大部分业务场景下是不能接受，那有没有更好的办法呢？ single-transaction 实现数据库备份InnoDB利用了“所有数据都有多个版本”的这个特性，实现了“秒级创建快照”的能力。 我们可以通过一致性视图实现数据库备份。当我们使用InnoDB时，倘若我们将事务隔离级别设置成可重复读。事务在启动的时候基于整库进行”快照”。因此我们便可以对当前数据进行备份，并不影响其他事务操作。 mysql官方自带的逻辑备份工具 mysqldump。 当mysqldump使用参数–single-transaction的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于MVCC的支持，这个过程中数据是可以正常更新的。但这又存在一个问题。它需要数据库存储引擎处于一致性读的事务隔离级别。倘若使用MYISAM，则不能使用该功能。 readonly 和 FTWRLmysql可以设置set global readonly=true将数据库变成全库只读。但他两的适用范围却不一样： 在有些系统中，readonly的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。因此，修改global变量的方式影响面更大。 在异常处理机制上有差异。如果执行FTWRL命令之后由于客户端发生异常断开，那么MySQL会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为readonly之后，如果客户端发生异常，则数据库就会一直保持readonly状态，这样会导致整个库长时间处于不可写状态，风险较高。 表级锁MySQL里面表级别的锁有两种：表锁和元数据锁（meta data lock，MDL)。 表锁MyISAM存储引擎只支持表锁。 表锁的语法是 lock tables … read/write。通过 unlock tables主动释放锁，也可以在客户端断开的时候自动释放。 当一个线程获取到表级写锁后，只能由该线程对表进行读写操作，别的线程必须等待该线程释放锁以后才能操作 当一个线程获取到表级读锁后，该线程只能读取数据不能修改数据，其它线程也只能加读锁，不能加写锁 MDLMDL在MySQL 5.5版本中引入。MDL不需要显式使用，在访问一个表的时候会被自动加上。 当对一个表做增删改查操作的时候，加MDL读锁； 当要对表做结构变更操作的时候，加MDL写锁。 读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。 申请MDL锁的操作会形成一个队列，队列中写锁获取优先级高于读锁。一旦出现写锁等待，不但当前操作会被阻塞，同时还会阻塞后续该表的所有操作。事务一旦申请到MDL锁后，直到事务执行完才会将锁释放。 如果A会话开启一个事务对t表进行了一次查询并没有提交，该操作会获取MDL读锁。然后B会话对t表字段进行更改，上一个事务没有提交MDL读锁没有被释放，因此B会话block。但是之后所有要在表t上新申请MDL读锁的请求也会被B会话阻塞。 如何安全地给小表加字段？首先我们要解决长事务，事务不提交，就会一直占着MDL锁。在MySQL的information_schema 库的 innodb_trx 表中，你可以查到当前执行中的事务。如果你要做DDL变更的表刚好有长事务在执行，要考虑先暂停DDL，或者kill掉这个长事务。倘若要变更的表是一个热点表，虽然数据量不大，但是上面的请求很频繁，这时候kill可能未必管用，因为新的请求马上就来了。比较理想的机制是，在alter table语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到MDL写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。之后开发人员或者DBA再通过重试命令重复这个过程。 Online DDLmysql 5.6以前 在DDL执行期间其他DML不能并行，运维人员对表的结构更改会导致该表的业务停滞。这在很多时候是不能容忍的，因此在mysql5.6 引入了Online DDL ,它使得在DDL执行期间其他DML可以并行。 Online DDL下，当获取到MDL写锁会将其降级成MDL读锁，然后真正做DDL操作。因此在DDL操作的过程中，表处于读锁状态。DML不被阻塞。DDL操作完成后会升级成MDL写锁然后释放MDL锁。 行锁行锁是粒度最小的锁。在Innodb引擎中既支持行锁也支持表锁，但只有通过索引条件检索数据，InnoDB才使用行级锁，否则，InnoDB将使用表锁。行锁开销会增大，加锁速度慢，并会出现死锁。但因为锁粒度变小，发生锁冲突的概率低，处理并发的能力强。 种类共享锁（S锁）: 也称读锁，允许其他事物再加S锁，不允许其他事物再加X锁 排他锁（X锁）: 也称写锁，不允许其他事务再加S锁或者X锁 加锁方式自动加锁对于UPDATE、DELETE和INSERT语句，InnoDB会自动给涉及数据集加排他锁；对于普通SELECT语句，InnoDB不会加任何锁； 手动加锁普通SELECT语句是不会加锁的，但是也可以手动加锁。 共享锁：select…lock in share mode 排他锁：select … for update 明明普通SELECT查询数据，读写不冲突塞，为什么还要对其上锁呢？ 因为mysql读操作其实分为2种类型:快照读（snapshot read）及当前读（current read）。前者基于Mysql mvcc实现,后者是对数据库当前数据块的读取。 UPDATE、INSERT、DELETE 和 加锁的SELECT 都是当前读，而普通的SELECT则是快照读。 next-key lock行锁只能锁住行，并不能锁住后面加入满足条件的行，这是幻读产生的原因。如果能对一个范围进行加锁，保证这个范围内数据不变，就可以消除幻读的问题。间隙锁（Gap Lock） 则是Innodb在可重复读隔离级别下提供的一种锁定范围左闭右闭的区间锁。Innodb通过行锁和间隙锁共同组成的（next-key lock）从而解决了幻读问题。 可重复读隔离级别下加锁规则： 加锁的基本单位是next-key lock（前开后闭区间）。 查找过程中访问到的对象才会加锁。 索引上的等值查询，给唯一索引加锁的时候，next-key lock退化为行锁。 索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock退化为间隙锁。 参考： 间隙锁详解MySQL实战45讲]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>lock</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python collections 深入解析]]></title>
    <url>%2F2020%2F09%2F05%2F1%2F</url>
    <content type="text"><![CDATA[collections是Python内建的一个集合模块，提供了许多有用的集合类。 名称 说明 namedtuple 创建命名元组子类的工厂函数 deque 类似列表(list)的容器，实现了在两端快速添加(append)和弹出(pop) ChainMap 类似字典(dict)的容器类，将多个映射集合到一个视图里面 Counter 字典的子类，提供了可哈希对象的计数功能 OrderedDict 字典的子类，保存了他们被添加的顺序 defaultdict 字典的子类，提供了一个工厂函数，为字典查询提供一个默认值 UserDict 封装了字典对象，简化了字典子类化 UserList 封装了列表对象，简化了列表子类化 UserString 封装了列表对象，简化了字符串子类化 UserDict,UserList,UserString都是对于dict,list，str的封装，本质是一样的。因为底层的内容可以作为属性来访问,一般用作继承从而实现自定义功能，不做过多概述。 ChainMap在Python中，当我们有两个字典需要合并的时候，可以使用字典的update方法，但这会改变其中一个字典。如果我们不想改变原有的两个字典，那就需要单独再创建一个字典，但这会额外内存。ChainMap可以在不修改原有字典前提下，又不另外创建一个新的字典，读写这个对象就像是读字典一样。实际上ChainMap存储的是字典的引用。在其内部会储存一个Key到每个字典的映射，当你读取e[key]的时候，它先去查询这个key在哪个字典里面，然后再去对应的字典里面查询对应的值。 使用ChainMap需知: 如果两个字典里面有一个Key的名字相同，那么ChainMap会使用第一个拥有这个Key的字典里面的值 如果为ChainMap对象添加一个Key-Value对，新的Key-Value会被添加进第一个字典里面 如果修改原字典，修改的key在第一个字典或则在多个字典中只有一个，那么ChainMap对象也会相应更新。如果这个Key在多个字典中都存在，且修改的不是第一个，则不更新。 如果从ChainMap对象里面删除一个Key，如果这个Key只在一个源字典中存在，那么这个Key会被从源字典中删除。如果这个Key在多个字典中都存在，那么Key会被从第一个字典中删除。当被从第一个字典中删除以后，第二个源字典的Key可以继续被ChainMap读取。 基本实现如下1234567891011121314151617181920class ChainMap(MutableMapping): def __init__(self, *maps): self.maps = list(maps) or [&#123;&#125;] def __getitem__(self, key): for mapping in self.maps: try: return mapping[key] except KeyError: pass return self.__missing__(key) def __setitem__(self, key, value): self.maps[0][key] = value def __delitem__(self, key): try: del self.maps[0][key] except KeyError: raise KeyError('Key not found in the first mapping: &#123;!r&#125;'.format(key)) CounterCounter是dict的子类，用于计数可哈希对象。它是一个集合，元素像字典键(key)一样存储，它们的计数存储为值。计数可以是任何整数值，包括0和负数。 主要实现逻辑在update方法中: 12345678910111213141516171819202122232425262728293031323334353637class Counter(dict): def __init__(*args, **kwds): if not args: raise TypeError("descriptor '__init__' of 'Counter' object " "needs an argument") self, *args = args if len(args) &gt; 1: raise TypeError('expected at most 1 arguments, got %d' % len(args)) super(Counter, self).__init__() self.update(*args, **kwds) def update(*args, **kwds): if not args: raise TypeError("descriptor 'update' of 'Counter' object " "needs an argument") self, *args = args if len(args) &gt; 1: raise TypeError('expected at most 1 arguments, got %d' % len(args)) iterable = args[0] if args else None if iterable is not None: if isinstance(iterable, Mapping): if self: self_get = self.get for elem, count in iterable.items(): self[elem] = count + self_get(elem, 0) else: super(Counter, self).update(iterable) # fast path when counter is empty else: _count_elements(self, iterable) if kwds: self.update(kwds) def _count_elements(mapping, iterable): 'Tally elements from the iterable.' mapping_get = mapping.get for elem in iterable: mapping[elem] = mapping_get(elem, 0) + 1 通过源码分析：Counter可以传入iterable或则Mapping。当传入 iterable时，会调用_count_elements，遍历累加元素个数当传入 Mapping时，会遍历累加self[elem] = count + self_get(elem, 0)，通过源码可以发现，传入的Mapping的value值必须为整数。 dequedeque作为双端队列,其底层实现基于双向链表,其插入和删除效率远高于list,因此可以用来实现栈（stack）也可以用来实现队列（queue）。 双向链表实现:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121class Node(object): def __init__(self, value=None, prev=None, next=None): self.value, self.prev, self.next = value, prev, next def __str__(self): return f"&#123;self.__class__&#125;: &#123;self.value&#125;"class CircularDoubleLinkedList(object): def __init__(self, maxsize=20): self.maxsize = maxsize node = Node() node.next, node.prev = node, node self.root = node self.length = 0 def __len__(self): return self.length def __str__(self): return f"&#123;self.__class__&#125;: &#123;self.tolist()&#125;" @property def headnode(self): return self.root.next @property def tailnode(self): return self.root.prev def append(self, value): self.is_full() node = Node(value, prev=self.tailnode, next=self.root) self.tailnode.next = node self.root.prev = node self.length += 1 def pop(self): tailnode = self.tailnode if tailnode == self.root: raise Exception("Empty") tailnode.prev.next = self.root self.root.prev = tailnode.prev self.length -= 1 return tailnode.value def pop_left(self): headnode = self.headnode if headnode == self.root: raise Exception("Empty") headnode.next.prev = self.root self.root.next = headnode.next self.length -= 1 return headnode.value def append_left(self, value): if self.root.next is self.root: return self.append(value) self.is_full() headnode = self.headnode node = Node(value, prev=self.root, next=headnode) self.root.next = node headnode.prev = node self.length += 1 def remove(self, node): if node is self.root: return node.prev.next = node.next node.next.prev = node.prev self.length -= 1 del node return 1 def iter_node_reverse(self): if self.root.prev is self.root: return node = self.root.prev while node is not None and node != self.headnode: yield node node = node.prev yield node def __iter__(self): for node in self.iter_node(): yield node.value def iter_node(self): if self.root.next is self.root: return node = self.root.next while node is not None and node != self.tailnode: yield node node = node.next yield node def tolist(self): return [i for i in self] def is_full(self): if len(self) &gt; self.maxsize: raise Exception("Full") namedtuplecollection.namedtuple能够方便地在Python中手动定义一个内存占用较少的不可变类。 类实现:1234class User: def __init__(self, name, age): self.name = name self.age = age namedtuple实现:1User = namedtuple("User", ["name", "age", "height"]) namedtuple转换为dict:1user_info_dict = user._asdict() namedtuple拆包:1name, age, *others = user defaultdictdefaultdict属于内建函数dict的一个子类，调用工厂函数提供缺失的值。 对比假如我们要对一个字符串字符计数 1234import randomimport string # 对_str字符计数 _str = ''.join(random.choices(string.ascii_lowercase, k=20)) usual: 123456counter = &#123;&#125;for s in _str: try: counter[s] += 1 except KeyError: counter[s] = 1 setdefault: 1234counter = &#123;&#125;for s in _str: counter.setdefault(s, 0) counter[s] += 1 defaultdict: 123counter = defaultdict(int)for s in _str: counter[s] += 1 原理defaultdict是怎么实现这一功能的呢，我们可以看一下UserDict源码的12345678```python def __getitem__(self, key): if key in self.data: return self.data[key] if hasattr(self.__class__, &quot;__missing__&quot;): return self.__class__.__missing__(self, key) raise KeyError(key) 可以看到当dict中不存在某一key时，会试图执行1234567891011121314因此我们可以自定义defaultdict:```pythonfrom collections import UserDictclass CustomDefaultDict(UserDict): def __init__(self, default_factory=None, *arg, **kwargs): super().__init__(*arg, **kwargs) self.default_factory = default_factory def __missing__(self, key): self[key] = value = self.default_factory() return value OrderedDictdict基于hash实现，具备优秀的插入效率和查询效率，但却无法做到有序插入。在一些需要插入顺序的场景下显得乏力。怎么做到，既保证其时序性的同时又具备很好的检索效率呢，我们可以设计一个链表，用于存储key插入的先后顺序。这就是OrderedDict的是实现原理。 源码解析OrderedDict在python中通过双向链表和dict实现。在python 3.6 版本中，dict默认具备插入顺序。 通过阅读源码我们可以看到，OrderedDict定义了两个实例变量12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152```self.__root```做为链表的头节点用于实现循环双向链表```self.__map```将字典的key与双向链表中节点进行关联借助这两个变量OrderedDict的实现就很简单了```pythonclass OrderedDict(dict): def __init__(*args, **kwds): if not args: raise TypeError(&quot;descriptor &apos;__init__&apos; of &apos;OrderedDict&apos; object &quot; &quot;needs an argument&quot;) self, *args = args if len(args) &gt; 1: raise TypeError(&apos;expected at most 1 arguments, got %d&apos; % len(args)) try: self.__root except AttributeError: self.__hardroot = _Link() self.__root = root = _proxy(self.__hardroot) root.prev = root.next = root self.__map = &#123;&#125; self.__update(*args, **kwds) def __setitem__(self, key, value, dict_setitem=dict.__setitem__, proxy=_proxy, Link=_Link): if key not in self: self.__map[key] = link = Link() root = self.__root last = root.prev link.prev, link.next, link.key = last, root, key last.next = link root.prev = proxy(link) dict_setitem(self, key, value) def __delitem__(self, key, dict_delitem=dict.__delitem__): dict_delitem(self, key) link = self.__map.pop(key) link_prev = link.prev link_next = link.next link_prev.next = link_next link_next.prev = link_prev link.prev = None link.next = None def __iter__(self): root = self.__root curr = root.next while curr is not root: yield curr.key curr = curr.next LRU算法实现在有限的空间中存储对象时，当空间满时，会按照一定算法从有限空间删除部分对象。常用的算法有LRU，FIFO，LFU等。LRU：least recently used，最近最少使用算法。在计算机的Cache硬件，以及主存到虚拟内存的页面置换，还有Redis缓存系统中都用到了该算法。 在Python中，可以使用collections.OrderedDict很方便的实现LRU算法 123456789101112131415161718192021222324252627282930from collections import OrderedDict class LRUCache(OrderedDict): def __init__(self,capacity): self.capacity = capacity self.cache = OrderedDict() def get(self,key): if self.cache.has_key(key): value = self.cache.pop(key) self.cache[key] = value else: value = None return value def set(self,key,value): if self.cache.has_key(key): value = self.cache.pop(key) self.cache[key] = value else: if len(self.cache) == self.capacity: self.cache.popitem(last = False) self.cache[key] = value else: self.cache[key] = value 参考:https://juejin.im/post/6844903875711860750]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>collections</tag>
        <tag>linked list</tag>
        <tag>lru</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gevent魔法和test中的应用实践]]></title>
    <url>%2F2020%2F08%2F26%2F1%2F</url>
    <content type="text"><![CDATA[了解异步编程的同学对于gevent一定不会陌生,但大部分人对于gevent可以在不修改任何代码的前提下将原始同步代码更替成异步代码的原理知之甚少。很多人很胆怯阅读源码,但通过阅读源码，你既能学习到正确的编码规范，也能学习到好的编程思路。接下来我带领大家一起来揭开gevent这层神秘面纱，及借助gevent思想在项目test中的应用。 gevent猴子补丁gevent的用途是在不侵入原始代码的前提下，让你可以很方便的导入非阻塞的模块，无需特意引入。其实现原理是将原始库的sync func替换成gevent内置的 async func。 模块补丁实现暴力替换:在python sys变量中将目标模块直接替换成新的模块123# module_1.pydef print1(): print("print 1") 123# module_2.pydef print2(): print("print 2") 1234567891011# test.pyimport sysimport module_1module_1.pprint()del sys.modules['module_1']sys.modules['module_1'] = __import__('module_2')import module_1module_1.pprint() output:12print 1print 2 非暴力替换:将目标模块特定func 替换成新模块的特定func 12345678910import module_1import module_2def monkey_patch(): module_1.pprint = module_2.pprintmonkey_patch()module_1.pprint() output:1print 2 从上面两个例子中你可以很清晰的看到虽然调用的是目标函数,但实际上执行的确实被替换的新的函数。 gevent monkey.parch_all源码解析patch_all 就是用gevent的async的func替换python 原生库 sync的funcpatch_all针对模块的引入是有顺序的, 因为他们之间有依赖的关系。 1234567891011121314151617181920def patch_all(socket=True, dns=True, time=True, select=True, thread=True, os=True, ssl=True, httplib=False, # Deprecated, to be removed. subprocess=True, sys=False, aggressive=True, Event=True, builtins=True, signal=True, queue=True, **kwargs): # locals() 函数会以字典类型返回当前位置的全部局部变量。 _warnings, first_time, modules_to_patch = _check_repatching(**locals()) # 存在 warning 并且不是第一次加载 return if not _warnings and not first_time: return # 省略 ... # 内置对于python原生库的封装，进行sync向aysnc的替换操作 if os: patch_os() if time: patch_time() # 省略 ... _check_repatching对patch_all 函数局部变量校验: 是否第一次加载 变量参数书否一一致 1234567891011121314saved = &#123;&#125;def _check_repatching(**module_settings): _warnings = [] key = '_gevent_saved_patch_all' del module_settings['kwargs'] # 前后加载的参数不一致 warning if saved.get(key, module_settings) != module_settings: _queue_warning("Patching more than once will result in the union of all True" " parameters being patched", _warnings) first_time = key not in saved saved[key] = module_settings return _warnings, first_time, module_settings _patch_module获取gevent内置的module和目标module 12345678910111213141516@_ignores_DoNotPatchdef patch_os(): _patch_module('os') def _patch_module(name, items=None, _warnings=None, _notify_did_subscribers=True):gevent_module = getattr(__import__('gevent.' + name), name)module_name = getattr(gevent_module, '__target__', name)target_module = __import__(module_name)patch_module(target_module, gevent_module, items=items, _warnings=_warnings, _notify_did_subscribers=_notify_did_subscribers)return gevent_module, target_module patch_module 获取target_module需要被替换的attr list 可以通过 items 指定，也可以在source_module的变量（ implements ）中写入。然后调用patch_item进行函数替换操作。 123456789101112131415161718192021_NONE = object()def patch_module(target_module, source_module, items=None, _warnings=None, _notify_did_subscribers=True): if items is None: items = getattr(source_module, '__implements__', None) if items is None: raise AttributeError('%r does not have __implements__' % source_module) for attr in items: patch_item(target_module, attr, getattr(source_module, attr))def patch_item(module, attr, newitem): olditem = getattr(module, attr, _NONE) if olditem is not _NONE: saved.setdefault(module.__name__, &#123;&#125;).setdefault(attr, olditem) setattr(module, attr, newitem) 以上就是gevent 通过 内置的async func 替换 python 内置target sync func 的基本流程，是不是也并不难理解。 test中的启发工作中,为了保证代码质量，避免出现一想不到的错误，编写测试用例是在开发过程中视为不可忽视的一环，甚至还出现了TDD等开发流程。 测试一般都是本着不侵入目标代码的前提下，对目标代码功能进行测试。 但很多时候不避免的就会需要在目标代码上做一些微调来适配测试。如:项目中使用了request,但是在测试环境下，不需要真正的请求发送，可以参考httmock。 倘若要满足不侵入的前提条件，就需要mock出一个服务。在测试阶段使用mock的服务。 本人从事的项目还参与硬件的交互。项目中内置了一个模块封装与设备的交互。上层业务代码只需要调用模块的func即可。但是在test时，存在一个难点: 需要有已经连接的设备。 怎么剔除这个依赖呢? 装饰器替换将装饰器作用于被调用模块的class或则func，替换目标函数 简单demo:12345678910111213141516171819def pprint(x): print(222, x)def deco(item): def inner(): setattr(item, "pprint", pprint) return item return inner@decoclass TT: def pprint(self, x): print(111, x)TT().pprint(233) output:1222 233 缺点： 存在侵入代码的情况 对于源码库或则第三方代码并不能直接加装饰器，需要做一层封装后才能实现 gevent方案进行替换如下目录结构1234567├── app│ └── libs│ └── http_client.py└── mock └── monkey └── http_client.py└── test.py 模块替换代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162# mock.monkey.pyimport importlibsaved = &#123;&#125;_NONE = object()def patch_all(httpclint=True, **kwargs): first_time, modules_to_patch = _check_repatching(**locals()) if not first_time: return if httpclint: patch_httpclint()def _check_repatching(**module_settings): key = '_gevent_saved_patch_all' del module_settings['kwargs'] first_time = key not in saved saved[key] = module_settings return first_time, module_settingsdef patch_httpclint(): _patch_module('http_client')def _patch_module(name, items=None, _warnings=None, _notify_did_subscribers=True): gevent_module = importlib.import_module('mock.' + name) module_name = getattr(gevent_module, '__target__', name) target_module = importlib.import_module(module_name) patch_module(target_module, gevent_module, items=items, _warnings=_warnings, _notify_did_subscribers=_notify_did_subscribers) return gevent_module, target_moduledef patch_module(target_module, source_module, items=None, _warnings=None, _notify_did_subscribers=True): if items is None: items = getattr(source_module, '__implements__', None) if items is None: raise AttributeError('%r does not have __implements__' % source_module) for attr in items: patch_item(target_module, attr, getattr(source_module, attr)) return Truedef patch_item(module, attr, newitem): olditem = getattr(module, attr, _NONE) if olditem is not _NONE: saved.setdefault(module.__name__, &#123;&#125;).setdefault(attr, olditem) setattr(module, attr, newitem) 12345678910111213# mock.monkey.py# 需要被替换的模块的位置__target__ = "app.libs.http_client"# 被替换模块的方法__implements__ = ['request']def request(method="GET", url="", **kwargs): if method == "GET": return 11111111 return 22222222 123456789# test.pyimport proxy.monkeyproxy.monkey.patch_all()from app.libs.http_client import requestprint(request(method="POST")) output:122222222 是不是很酷，非常优雅的弥补了装饰器的不足。在测试的时候开启替换，然后就可以规避硬件或则网络等复杂因素。也让测试变得更加高效起来 参考 捕蛇者说 http://xiaorui.cc/archives/3248]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>源码</tag>
        <tag>test</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[condition源码解析]]></title>
    <url>%2F2020%2F07%2F20%2F1%2F</url>
    <content type="text"><![CDATA[Condition是一种多线程通信工具，表示多线程下参与数据竞争的线程的一种状态，主要负责多线程环境下对线程的挂起和唤醒工作 实例化实例Condition时可以指定一个lock，如果没有指定，默认创建RLock的实例。同时Condition拥有与RLock一样的上锁方法acquire()和解锁方法release()。并初始化双端队列waiters，用于存放 wait的thread 123456789101112131415161718192021222324class Condition: def __init__(self, lock=None): if lock is None: lock = RLock() self._lock = lock # Export the lock's acquire() and release() methods self.acquire = lock.acquire self.release = lock.release # 如果锁定义了_release_save()或_acquire_restore()， # 这些覆盖默认实现(只是调用Rlock的release()和acquire())。_is_owned同上()。 try: self._release_save = lock._release_save except AttributeError: pass try: self._acquire_restore = lock._acquire_restore except AttributeError: pass try: self._is_owned = lock._is_owned except AttributeError: pass self._waiters = _deque()` 实现上下文语法12345def __enter__(self): return self._lock.__enter__()def __exit__(self, *args): return self._lock.__exit__(*args) 初探notify和wait条件锁的两个重要方法是notify()和wait()。notify()和wait()必须在条件锁上锁的状态下使用notify()和wait()被调用，程序会先去调用self._is_owned()，判断当前线程号与条件锁中的self._ower是否一致，如果不一致，抛出异常RuntimeError。1234567def _is_owned(self): # 如果当前线程拥有锁，_lock.acquire(0)返回False if self._lock.acquire(0): self._lock.release() return False else: return True wait()先回释放第一层Rlock,其他线程就可以获取锁执行，然后会生成一个Lock,写入waiter并将自己锁住，等待其他线程释放 1234567891011121314151617181920212223242526272829303132333435def wait(self, timeout=None): if not self._is_owned(): raise RuntimeError("cannot wait on un-acquired lock") # 创建一个锁对象Lock，并获取它，然后把它放到waiting池，需要可以被其他线程释放 waiter = _allocate_lock() waiter.acquire() self._waiters.append(waiter) # 释放底层锁，并保存锁对象的内部状态 saved_state = self._release_save() gotit = False try: if timeout is None: # 如果timeout是None，那么再次以阻塞的方式获取锁对象 # 当前线程已经获取了一次该锁，再次acquire()当前线程会阻塞，直到其他线程释放该锁 waiter.acquire() gotit = True else: # 如果timeout不是None，那么重复下面的流程： # time &gt; 0 阻塞方式获取锁,timeout时间内block # time &lt;0 以非阻塞方式获取锁 if timeout &gt; 0: gotit = waiter.acquire(True, timeout) else: gotit = waiter.acquire(False) return gotit finally: # 重新上第一层锁 self._acquire_restore(saved_state) if not gotit: try: # 如果因为超时，而不是被唤醒，退出的wait()，那么将锁从waiting池中移除 self._waiters.remove(waiter) except ValueError: pass notify()是在一个双端队列中进行操作，这个队列在Condition中名为_waiters。默认情况下，notify只会释放一个锁（按先进先出原则）。如果队列中没有锁，直接退出函数，不报任何异常。 12345678910111213def notify(self, n=1): if not self._is_owned(): raise RuntimeError("cannot notify on un-acquired lock") all_waiters = self._waiters waiters_to_notify = _deque(_islice(all_waiters, n)) if not waiters_to_notify: return for waiter in waiters_to_notify: waiter.release() try: all_waiters.remove(waiter) except ValueError: pass Condition中还有一个notify_all()方法，调用它会释放队列中全部的锁。 1234def notify_all(self): self.notify(len(self._waiters))notifyAll = notify_all 总结condition 非常重要，在Queue,Semaphore,ThreadExecutorPool都有应用。 condition实现主要依靠两层锁： condition初始化时创建一把锁(外部锁)，使用时需要先对外部锁上锁 每次调用wait时，会先生成一个lock锁(内部锁)，将内部锁放到算双端队列waiters中， 然后上锁，再将外部锁释放。并再次获取内部锁block,等待其他线程调用notify释放该内部锁， 然后该线程会去试图获取内部锁。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>condition</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rlock源码解析]]></title>
    <url>%2F2020%2F07%2F07%2F1%2F</url>
    <content type="text"><![CDATA[可重入锁RLock允许在同一线程中被多次acquire，我们将一一揭晓它的实现过程。 Lock123456import _thread_allocate_lock = _thread.allocate_lockLock = _allocate_lock Lock只是简单的对底层库做了个代理。锁并不属于锁定它的线程;另一个线程可以解锁它。由C代码实现。 12345678910111213141516171819202122232425import threadingimport timedef worker1(lock): lock.acquire() print("worker1 running") time.sleep(5) lock.release() passdef worker2(lock): lock.release() print("worker2 running") time.sleep(2) lock.acquire() passlock = threading.Lock()t = threading.Thread(target=worker1, args=(lock,))t1 = threading.Thread(target=worker2, args=(lock,))t.start()t1.start() worker2可以释放worker1获得的锁，获得的锁也能被worker1释放 RLock两个特性: 可重入:持有锁的线程可以多次acquire,release 线程独占:当前线程获取的锁其他线程不能释放 12345678def RLock(*args, **kwargs): """ 该工厂函数返回一个新的可重入锁。 一个可重入锁必须由创建它的线程释放。一旦一个线程获得了一个可重入锁，该线程可用无阻塞的再次获取。每次获取锁后必须进行释放。 """ if _CRLock is None: return _PyRLock(*args, **kwargs) return _CRLock(*args, **kwargs) 在3.6.5版本中。默认采用的是C 语言实现的，即_CRLock。还有一个Python版本实现的，即_PyRLock（也就是类_RLock） 123456class _RLock: def __init__(self): self._block = _allocate_lock() self._owner = None self._count = 0 通过查看Rlock 实例化可以得知，Rlock只是在Lock基础上另外维护了_owner,_count从而达到-可重入的效果； _block :内部互斥排他锁同Lock_owner :记录RLock锁的持有者的线程id,判断锁的持有者和当前的线程是否一致_count : 内部的计数器，用来记录锁被持有的次数，对于RLock的属主线程，每获取一次锁就+1，反之每释放一次锁就-1，当计数器为0时，就释放内部的锁，这样其他线程可以继续获取该内部锁，继而获取了这个Rlock 1234567891011121314151617def acquire(self, blocking=True, timeout=-1): me = get_ident() if self._owner == me: # 如果当前线程的pid是RLock对象所在的线程，那么对计数器进行加一操作 self._count += 1 return 1 # 不满足上述条件： # 1. 当前线程非RLock对象所在线程 # 2. RLock对象还未持有锁，即 self.owner = None # 对Lock上锁： # 当 blocking=True 时，当前线程被阻塞，直到持有锁的线程将锁释放后，rc = True # 当 blocking=False 时，可以非阻塞的获取。如果获取锁成功，rc = True；获取失败，rc = False rc = self._block.acquire(blocking, timeout) if rc: self._owner = me self._count = 1 return rc acquire方法的流程图如下: 1234567def release(self): if self._owner != get_ident(): # 如果持有锁的线程非当前线程，则抛异常 raise RuntimeError("cannot release un-acquired lock") self._count = count = self._count - 1 if not count: # 如果计数器减到0，那么释放RLock内部的锁，此时其他线程就可以获取到锁 self._owner = None self._block.release() release方法的流程图如下: 参考： https://blog.csdn.net/u010301542/article/details/80144643 https://reishin.me/python-source-code-parse-with-rlock/ https://www.cnblogs.com/buwu/p/12740646.html]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>源码</tag>
        <tag>lock</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python中的多继承]]></title>
    <url>%2F2020%2F04%2F26%2F1%2F</url>
    <content type="text"><![CDATA[相信学过java的同学们都知道java并不支持多继承，但python中却是支持多继承的，而且像python社区中非常有名的框架中就疯狂引入多继承来扩展类的功能。python多继承基于mro算法来实现的。 MRO 发展史二义性问题：当b1,b2都继承A时，假设A类有个f()方法都被b1,b2重写。此时c在同时继承b1,b2当调用f()方法时，就无法确定调用那个方法。 python 为了处理二义性问题，Python采用MRO机制。在python发展过程也逐步对MRO算法进行优化。 before python2.2Python2.2以前的版本：经典类 MRO算法依据:DFS DFS(深度优先算法)采用递归的形式，用到了栈结构，先进后出，存在一定缺陷。 在正常继承模式下，符合预期，但是在棱形继承模式下C类即便继承D类的方法，但由于D类在DFS下先于C类，因此C类的方法永远不会被执行 python2.2python2.2开始引入新式类，经典类依旧采用DFS,而新式类采用BFS python新式类和经典类的区别 BFS(广度优先算法)选取状态用队列的形式，先进先出，依旧存在缺陷。虽然棱形继承模式下的缺陷被解决了，但正常继承模式下显得不太常规。 python2.3~2.7从python2.3开始python对新式类的MRO算法进行调整,采用C3算法。一举解决了DFS BFS 存在的局限。 after python3python3开始,python放弃了经典类，所有的类都是新式类采用C3算法。 总结 python版本 MRO算法依据 before python2.2 DFS(经典类) python2.2 DFS(经典类),BFS(新式类) python2.3~2.7 DFS(经典类),C3(新式类) python3 C3(新式类) C3 线性算法原理我们用 C1C2⋯CN表示包含 N 个类的列表，并令head(C1C2⋯CN)=C1tail(C1C2⋯CN)=C2C3⋯CN假设类 C继承自父类 B1,⋯,BNB1,⋯,BN，那么根据 C3 线性化，类 C的方法解析列表通过如下公式确定：L[C(B1⋯BN)]=C+merge(L[B1],⋯,L[BN],B1⋯BN) 该操作可以分为以下几个步骤:1.选取 merge 中的第一个列表记为当前列表 K。2.令 h=head(K) ，如果 h没有出现在其他任何列表的 tail 当中，那么将其加入到类 C 的线性化列表中，并将其从 merge 中所有列表中移除，之后重复步骤 2。3.否则，设置 KK 为 merge 中的下一个列表，并重复 2 中的操作。4.如果 merge 中所有的类都被移除，则输出类创建成功；如果不能找到下一个 h，则输出拒绝创建类 CC 并抛出异常。 算法演算如下： 123456789101112L[E] = L[E(O)] = E + merge(L[O],O) = [E,O] L[D] = [D,O] L[F] = [F,O] L[B] = B + merge(L[E],L[D],E,D) = B + merge([E,O],[D,O],E,D) = [B,E,D,O] L[C] = C + merge(L[D],L[F],D,F) = [C,D,F,O] L[A] = A + merge([B,E,D,O],[C,D,F,O],B,C) = [A,B,E,C,D,F,O] 查看MRO 继承关系： __MRO__ 以下是本人之前写的ppt,提供参考: MRO_PPT]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>mro</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis引用之计数器]]></title>
    <url>%2F2020%2F04%2F03%2F1%2F</url>
    <content type="text"><![CDATA[当我们的网站上线之后,我们很多时候需要对网站浏览记录进行分析，从而做出对策。如:我们需要对热点页面进行缓存，因此我们就需要知道页面点击数等等。由于redis的处理命令做到线程安全，并且支持大量的读写操作，因此将计数器储存到redis里面是一个非常好的方案。 设计思路我们需要记录不同时间精度下的用户点击数，首先相同精度只需存储单次，其实我们需要可以遍列表获取精度,显然需要具备去重并且排序的特性。这里我们选择zset用于存储精度数据。将分值设为0，使其根据成员名进行排序。1234567# key：known# type:zset1:hits 05:hits 060:hits 060*60:hits 0 而对于某一个精度下的点击数量的记录，我们采用hash 来进行存储，key 为时间戳可以保证唯一性，对value 进行incr ,由于redis 的单线程特性，已经保证对操作的原子性。 1234567# key: count:1:hits # type:hashtime1 70time2 40time3 50time4 50 对计数器进行更新通过pipeline既可以减少通信次数，还可以保证这些命令执行过程中不会插入其他命令。 1234567891011121314151617181920212223import timePRECISION = (1, 5, 60, 5 * 60, 60 * 60, 5 * 60 * 60, 24 * 60 * 60,)def update_counter(conn, name, count=1, now=None): """ :param conn: redis连接对象 :param name: 统计名(点击量，销量...) :param count: 访问数 :param now: 当前时间 :return: """ now = now or time.time() pipe = conn.pipeline() for prec in PRECISION: # 取得当前精度的开始时间 pnow = now // prec * prec hash = f'&#123;prec&#125;:&#123;name&#125;' pipe.zadd('known:', &#123;hash: 0&#125;) pipe.hincrby('count:' + hash, pnow, count) pipe.execute() 获取指定精度的数据通过获取precision精度hash 表下的所有数据，并对时间戳进行排序并格式化，展示给用户。 123456def get_counter(conn, name, precision, data_format="%Y-%m-%d %H:%M:%S"): hash = f'&#123;precision&#125;:&#123;name&#125;' all_counter = conn.hgetall('count:' + hash) return sorted( map(lambda obj: (time.strftime(data_format, time.localtime(int(obj[0]))), int(obj[1])), all_counter.items()), key=lambda obj: (obj[0], obj[1],)) 清理旧计数器在redis中，针对zset并没有对于其内部key对应的expire操作，因此需要提供一个删除解决方案。 需要注意以下几点： 在删除过程中，随时可能有新的计数器更新或添加进来，因此在删除计数器的时候需要使用redis事务。 针对一些更新频率过长的计数器，可以降低清理频率。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263QUIT = Falsedef clean_counters(conn): pipe = conn.pipeline(True) # 程序清理操作执行的次数,每执行一次加一 passes = 0 # 持续地对计数器进行清理，直到退出为止。 while not QUIT: # 记录清理操作开始执行的时间，用于计算清理操作执行的时长。 start = time.time() # 作为遍历 known(zset)表 value 的 index index = 0 while index &lt; conn.zcard('known:'): # 取特定精度的计数器表 key (hash) hash = conn.zrange('known:', index, index) index += 1 if not hash: break hash = hash[0] # 取得计数器的精度。 prec = int(hash.partition(':')[0]) # 通过精度计算清理的频率(精度为 1 min 以内的设置每次轮询都会清理，大于1min 设置 int(prec // 60)次进行清理) bprec = int(prec // 60) or 1 # 判断当前精度处于当前轮询次数下是否需要被清理 if passes % bprec: continue hkey = 'count:' + hash # 计算该 间 cutoff = time.time() - SAMPLE_COUNT * prec # 获取该精度下所有样本并排序 samples = sorted(map(int, conn.hkeys(hkey))) # 通过二分查找已经排序的列表中过期的key remove = bisect.bisect_right(samples, cutoff) if remove: # 移除过期计数样本 conn.hdel(hkey, *samples[:remove]) # 判断计数样本是否全部过期，过期删除在zset中的内容 if remove == len(samples): try: # 在尝试修改计数器散列之前，对其进行监视。确保删除时有添加可以拒绝删除操作 pipe.watch(hkey) # 验证计数器散列是否为空，如果是的话，那么从记录已知计数器的有序集合里面移除它 if not pipe.hlen(hkey): pipe.multi() pipe.zrem('known:', hash) pipe.execute() # 删除一个计数器，zset的zcard减一,因此index不变即可获取下一个精度 index -= 1 else: # 计数器散列并不为空，继续让它留在记录已有计数器的有序集合里面 pipe.unwatch() # 删除过程中有其他程序向这个计算器散列添加了新的数据，继续让它留在记录已知计数器的有序集合里面 except WatchError: pass # 清理次数加一 passes += 1 duration = min(int(time.time() - start) + 1, 60) # 如果这次循环未耗尽60秒钟，那么在余下的时间内进行休眠； # 如果60秒钟已经耗尽，那么休眠一秒钟以便稍作休息。 time.sleep(max(60 - duration, 1)) 以上就是通过redis 实现一个简单的计数器功能。]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis数据结构底层_String]]></title>
    <url>%2F2020%2F03%2F25%2F1%2F</url>
    <content type="text"><![CDATA[String在Redis中是可变字节数组的形式存在,但它并不是C所提供的原始字符数组。Redis通过自建Simple dynamic string(SDS)实现。 SDSredis在内部存储string都是用sds的数据结构实现。结构源码如下:12345678struct sdshdr&#123; //记录buf数组中已使用字节的数量,等于SDS保存字符串的长度 int len; //记录buf数组中未使用字节的数量 int free; //字节数组,用于保存字符串 char buf[]; &#125; redis的数据存储过程中为了提高性能，内部做了很多优化。string 内部还被拆分成三中编码: int编码: 存储字符串长度小于20且能够转化为整数的字符串 embstr编码: 保存长度小于44字节的字符串(redis3.2版本之前是39字节，之后是44字节) raw编码: 保存长度大于44字节的字符串(redis3.2版本之前是39字节，之后是44字节) embstr编码的结构图：raw编码的结构: 上面结构图可以看出embstr和raw都是由redisObject和sds组成的。但是embstr的redisObject和sds是连续的,只需要使用malloc分配一次内存;而raw需要为redisObject和sds分别分配内存，即需要分配两次内存。因此数据量小的情况下，embstr显然更加效率。但是当数据量变大,free不足,embstr因为是连续存储地址,需要对redisObject和sds重新分配,而raw只需要sds分别分配内存。因此redis会将embstr转化成raw从而实现扩容。 Redis会自动对编码进行转换来适应和优化数据的存储,对 int(加减除外) embstr 的修改(append,setrange…)，redis会先将其转换成raw，然后才进行修改。所以embstr实际上是只读性质的。 扩容策略： - string 容量 &lt; 1 M ,扩大一倍 - string 容量 &gt; 1 M ,扩大 1 M - string 容量 MAX : 512M 参考 https://redis.io/topics/internals-sds http://blog.itpub.net/69918724/viewspace-2647282/ https://database.51cto.com/art/201906/598234.htm https://www.lagou.com/lgeduarticle/16158.html https://www.jianshu.com/p/160fb0f73841]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解python装饰器]]></title>
    <url>%2F2020%2F01%2F16%2F1%2F</url>
    <content type="text"><![CDATA[装饰器在python中应用非常广泛，这种模式允许向一个现有的对象添加新的功能，同时又不改变其结构。一直以来我都装饰器的特性是可以将被装饰的函数替换成其他函数，实现功能的扩充。但最近阅读&lt;&lt;流畅的python&gt;&gt;的时候，才意识到其另一大特性是装饰器在加载模块是会立即执行。 演示通常，模块被加载时，模块内的装饰器就会被执行，和类变量的被执行方式一样。凭借这个特性，下面的结果就显得合理： 12345678910111213141516171819202122232425262728293031323334func_list = []def promotion(func): func_list.append(func) return func@promotiondef add(x, y): return x + y@promotiondef sub(x, y): return x - y@promotiondef mul(x, y): return x * y@promotiondef div(x, y): return x / ydef output(x, y): return &#123;func.__name__: func(x, y) for func in func_list&#125;if __name__ == '__main__': print(output(10, 2)) output: 1&#123;'add': 12, 'sub': 8, 'mul': 20, 'div': 5.0&#125; 扩展如果有参与过异步框架下的开发工作，肯定会出现底层未支持或无现成可用的异步package时，就会通过 run_in_executor让当前阻塞方式异步执行来避免异步框架因为局部阻塞导致并发严重下降。每一个方法都的重复此操作，未免太繁琐，装饰器就很好的可以解决这个问题123456def coroutine_executor(func): @wraps(func) def wrapper(*args, **kwargs): return asyncio.get_event_loop().run_in_executor(ThreadPoolExecutor(), partial(func, *args, **kwargs)) return wrapper 想要开始使用协程的时候必须通过next(…)方式激活协程，如果不预激，这个协程就无法使用，这种重复繁琐的事情交给装饰器再合适不过了1234567def coroutine(func): @wraps(func) def primer(*args,**kwargs): gen = func(*args,**kwargs) next(gen) return gen return primer 倘若有些方法执行耗时未知，但又不想改变其同步的运行模式，理论上在一定时间范围内可以接受。这时候，可以通过ThreadPoolExecutor和装饰器来实现这个功能。123456789def async_timeout(timeout=20): def decorate(func): @wraps(func) def wrapper(*args, **kwargs): return ThreadPoolExecutor().submit(partial(func, *args, **kwargs)).result(timeout=timeout) return wrapper return decorate 最近发现一个开源的package timeout-decorator,浏览了一下源码，就一个文件，写的非常详尽可以浏览使用。但年代感沉重，但还是挺有借鉴价值的。以上就是在我开发过程中通过装饰器带来的便利。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>装饰器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[annual_summary]]></title>
    <url>%2F2020%2F01%2F15%2F1%2F</url>
    <content type="text"><![CDATA[2019已悄然离去，回顾这一整年，思考良多。2019年也是正式步入社会的一年，在技术领域，在团队协作方面都有很多提升，话不多说，切入正题。 2018年下半年以小萌新的姿态从校园步入社会，因公司技术栈需求，从Java 转向 python,并逐渐迷恋这款方便快捷，语义清晰的语言。在2019年，随着python语言的深入，也开展了很多相关工作。起初接触了公司api开发，对django有了一定了解。随后转向业务开发，受迫于初创公司设计文档，注释严重匮乏，逻辑复杂且编码风格层出不穷，吃劲苦口，也深刻意识到python这种动态的强类型解释语言在团队开发中，在缺少语言本生强校验的前提下，更需要制定一些措施来避免编写这类动态语言代码重构火葬场的现象，因此在个人日常编码过程中尽可能详尽的提供描述信息，并配合 Type Hint 做到尽可能减少团队开发中不必要的其他因素的负担。 公司项目在这一年里也经历了逐步演化的过程，我有幸参与其中，并主导了部分演进工作，受益颇多。起初后端业务相关的项目server使用 HTTPServer (python原生支持的方式),过于底层且功能单一，绝大多数功能都需要自己逐一实现，如需要满足高并发的场景还需要遵守WSGI APPLICATION 规范来搭配现在主流的WSGI SERVER，等等一系列因素使得维护成本增高，且吃力不讨好(python社区 明明就对 app 和 sever 进行拆分，让程序员专注解决自己的业务需求)，我们团队便着手筹划项目演进。现在主流的web server 主要分为两大类:同步框架和异步框架。当时主要考虑因素在不影响现有架构的前提下保证轻量且高并发。显然异步框架更能满足高并发需求，最终选择aiohttp作为服务器框架。接触到异步编程，对这个邻域有了更深的理解。但在当前环境下，其实异步框架生态还并不友好，底层支持也不太完善，而且和之前同步模式差异过大。导致在迁移过程过程繁琐。 随后老板便指派我调研并主导架构的演进，我参考框架和现有项目的适配程度和框架生态完善程度，结合主流架构设计思路，最终选择了 flask + redis + gunicorn + nginx + docker 的架构。这个过程接触到了很多之前尚未接触或略有了解的知识领域，也意识到了自己知识的浅显，求知的道路还很漫长。在此期间，还主导搭建了一个前端项目，采用 vue + iview + nginx + docker 的架构。得意于架构的精良设计和前端生态的不断完善，让我一个不怎么会写css html 的新手也能信手捏来。 当然，在团队协作上也有了很大的提升，在讨论问题的时候最好预先提供上下文，再提出问题的时候最好有深度的思考，在解决问题的时候也要有更高的扩展性。对任务要有优先级的划分，不至于手足无措，对大型任务要设定排期，保证工期不被延误。 总的来说，2019是转变的一年，从小萌新初入职场，到老油条车臣沙场。以上就是我对自己的一个总结。2020，fighting!!!]]></content>
      <categories>
        <category>summary</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[middleware]]></title>
    <url>%2F2019%2F06%2F13%2F1%2F</url>
    <content type="text"><![CDATA[middleware与Django的请求/响应生命周期挂钩，middleware中定义的钩子函数可以用于全局改变Django的输入或输出。 本文将从以下几点介绍Django的middleware： 什么时候使用 middleware middleware是怎么运作的 自定义 StackOverflowMiddleware 什么时候使用 middleware当你需要修改请求，例如被传送到view中的HttpRequest对象。 或者修改view返回的HttpResponse对象，亦或者对运行中的异常作通用处理，使用中间件是一个很好的解决方案。 本人之前学习javaWeb时接触到filter可以用来识别用户是否登录，Django中的middleware和这个功能非常相似，可以在请求调用视图前做请求处理，从而实现校验逻辑。 全局拦截统计一分钟访问页面数，对于访问次数过多的 IP 将其到黑名单 BLOCKED_IPS，并拒绝访问 1234class BlockedIpMiddleware(object): def process_request(self, request): if request.META['REMOTE_ADDR'] in getattr(settings, "BLOCKED_IPS", []): return http.HttpResponseForbidden('&lt;h1&gt;Forbidden&lt;/h1&gt;') 错误显示网站放到服务器上正式运行后，DEBUG改为了False，这样更安全，但是有时候发生错误我们不能看到错误详情，可以设置管理员看到的是错误详情，而正常用户则显示友好的报错信息。 12345678import sysfrom django.views.debug import technical_500_responsefrom django.conf import settingsclass UserBasedExceptionMiddleware(object): def process_exception(self, request, exception): if request.user.is_superuser or request.META.get('REMOTE_ADDR') in settings.INTERNAL_IPS: return technical_500_response(request, *sys.exc_info()) middleware是怎么运作的我们从浏览器发出一个请求 Request，得到一个响应后的内容 HttpResponse ，这个请求传递到 Django的过程如下： Django 提供了一下五种钩子函数：12345process_request(self,request)process_view(self, request, view_func, view_args, view_kwargs)process_template_response(self,request,response)process_exception(self, request, exception)process_response(self, request, response) 想要使用middleware，需要在setting中的MIDDLEWARE中注册从而进行激活。middleware可以存在依赖关系，所以在注册时，需要确定依赖关系，从而排列合适的顺序。 上面的方法，前两个方法是请求进来时要穿越的，而后三个方法是请求出去时要穿越的，在MIDDLEWARE注册的middleware，请求是顺序执行的，响应是逆序执行的。 总结中间件的定义：wsgi之后 urls.py之前 在全局操作Django请求和响应的模块 中间件的使用process_request(self, request)执行顺序：按照注册的顺序（在settings.py里面设置中 从上到下的顺序） 何时执行：请求从wsgi拿到之后返回值： 返回None，继续执行后续的中间件的process_request方法 返回response , 不执行后续的中间件的process_request方法 process_response(self, request, response)执行顺序：按照注册顺序的倒序（在settings.py里面设置中 从下到上的顺序） 何时执行：请求有响应的时候 返回值：必须返回一个response对象 process_view(self, request, view_func, view_args, view_kwargs)执行顺序：按照注册的顺序（在settings.py里面设置中 从上到下的顺序） 何时执行：在urls.py中找到对应关系之后 在执行真正的视图函数之前 返回值： 返回None，继续执行后续的中间件的process_view方法 返回response, process_exception(self, request, exception)执行顺序：按照注册顺序的倒序（在settings.py里面设置中 从下到上的顺序） 何时执行：视图函数中抛出异常的时候才执行 返回值： 返回None,继续执行后续中间件的process_exception 返回response， process_template_response(self, request, response)执行顺序：按照注册顺序的倒序（在settings.py里面设置中 从下到上的顺序） 何时执行：视图函数执行完，在执行视图函数返回的响应对象的render方法之前 返回值： 返回None,继续执行后续中间件的process_exception 返回response， Django调用 注册的中间件里面五个方法的顺序12345671. process_requesturls.py2. process_viewview3. 有异常就执行 process_exception4. 如果视图函数返回的响应对象有render方法,就执行process_template_response5. process_response 自定义 StackOverflowMiddleware本小节将会创建一个异常处理的Middleware，并接入StackOverflow APi 在Debug 模式下 终端返回最高票数的解决方案。 需要使用requests 和StackOverflow API和python3.6运行环境 12345678910111213141516171819202122232425262728293031323334# middleware.pyimport requestsfrom django.utils.deprecation import MiddlewareMixin__author__ = 'Darr_en1'class StackOverflowMiddleware(MiddlewareMixin): def process_exception(self, request, exception): intitle = f'&#123;exception.__class__.__name__&#125;: &#123;exception.args[0]&#125;' url = 'https://api.stackexchange.com/2.2/search' headers = &#123;'User-Agent': 'github.com/vitorfs/seot'&#125; params = &#123; 'order': 'desc', 'sort': 'votes', 'site': 'stackoverflow', 'pagesize': 3, 'tagged': 'python;django', 'intitle': intitle &#125; r = requests.get(url, params=params, headers=headers) questions = r.json() print('') for question in questions['items']: print(question['title']) print(question['link']) print('') 尾部追加，保证异常发生时第一个执行 1234# setting.pyif DEBUG: MIDDLEWARE += ['middleware.StackOverflowMiddleware', ] 参考： http://www.xuyasong.com/?p=927 https://www.cnblogs.com/bainianminguo/p/9440610.html https://www.cnblogs.com/bainianminguo/p/9440610.html https://docs.djangoproject.com/en/2.2/topics/http/middleware]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sort]]></title>
    <url>%2F2019%2F05%2F08%2F1%2F</url>
    <content type="text"><![CDATA[排序在编程中非常常见，python内置sort()和sorted()两中排序方式。 list.sort() sorted() 区别1234# python3 取消cmpsorted(iterable, key=None, reverse=False) list.sort(self, key=None, reverse=False) sort()与sorted()的不同在于，sort是在原位重新排列list， sort() 函数不需要复制原有列表，消耗的内存较少，效率也较高。 sorted()是产生一个新的列表，并可以接受任何形式的可迭代对象（包括不可变序列和生成器），并返回list，因此可以满足不同需求。 介绍参数用法大体一致 sorted()iterable：是可迭代类型;key：用列表元素的某个属性和函数进行作为关键字，有默认值，迭代集合中的一项; reverse：排序规则. reverse = True 或者 reverse = False，有默认值； return：list list.sort()sort() 是list的内置函数通过list.sort()调用 key：用列表元素的某个属性和函数进行作为关键字，有默认值，迭代集合中的一项; reverse：排序规则. reverse = True 或者 reverse = False，有默认值。 return：None 对str去重排序12345678910# sort()In [2]: str="cdsciolsdruikcadssv" ...: s = list(set(str)) ...: s.sort(reverse=False) ...: print("".join(s))acdiklorsuv# sorted()In [3]: print("".join(sorted(set(str),reverse=False)))acdiklorsuv sort 之魅给定一个只包含大小写字母，数字的字符串，对其进行排序，保证： 所有的小写字母在大写字母前面所有的字母在数字前面所有的奇数在偶数前面 123In [5]: s = "Sorting1234" ...: "".join(sorted(s, key=lambda x: (x.isdigit(), x.isdigit() and int(x) % 2 == 0, x.isupper(), x.islower(), x)))Out[5]: 'ginortS1324' 分析：key 用来决定在排序算法中 cmp 比较的内容，key 可以是任何可被比较的内容，lambda 函数将输入的字符转换为一个元组，然后 sorted 函数将根据元组（而不是字符）来进行比较，进而判断每个字符的前后顺序。 元组大小比较：将第一元组的第一项与第二元组的第一项进行比较;如果它们不相等，这是比较的结果，否则第二项被考虑，然后第三项，等等。 sort函数内部实现原理sort 内部通过Timsort实现，详情查看python sort函数内部实现原理]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>sort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 中的下划线]]></title>
    <url>%2F2019%2F05%2F07%2F1%2F</url>
    <content type="text"><![CDATA[python 作为动态语言，不同于java通过关键字修饰变量作用域，Python中，解释器通过读取单下划线和双下划线来实现不同的含义。 python中，通过定义以下五种下划线模式和命名约定： 单前导下划线：_var 单末尾下划线：var_ 双前导下划线：__var 双前导和末尾下划线： var 单下划线：_ 单前导下划线：_varpython中的单前导下划线在python中有一个约定俗成的含义，当它定义一个变量或则函数名时，表明该的变量或函数仅供内部使用。但这不是强制规定，外部依旧可以访问。 1234567# test1.py:def a(): return 23def _b(): return 42 外部调用通过通配符的方式调用不到单前导下划线定义的变量或函数，常规导入不受前导单个下划线命名约定的影响123456789&gt;&gt;&gt; from my_module import *&gt;&gt;&gt; a()23&gt;&gt;&gt; _b()NameError: "name '_b' is not defined"&gt;&gt;&gt; from my_module import _b&gt;&gt;&gt; _b()24 单末尾下划线 var_单下划线结尾的变量：用于避免于Python关键字冲突的变量，如class_： Tkinter.Toplevel(master, class_=’ClassName’) 双前导下划线 __var双下划线开头的变量：它在模块中还是当作单下划线看待，但出现在类中作为类属性就不一样了，不能直接访问.12345678In [3]: class A(object): ...: a = 40 ...: _a = 50 ...: __a = 60 # _Foo__boo ...: def __init__(self): ...: self.__b = 70 ...: def __test(self): ...: print("__test") 单前导下划线标识的类变量依旧可以被调用，但双前导下划线标识的变量或函数会被解释器隐藏12345678910In [4]: A._aOut[4]: 50In [5]: A.__a---------------------------------------------------------------------------AttributeError Traceback (most recent call last)&lt;ipython-input-9-aca55768bfd2&gt; in &lt;module&gt;----&gt; 1 A.__aAttributeError: type object 'A' has no attribute '__a' 通过dict可以查看对象属性，通过A._A__a可访问__a。1234567891011121314In [6]: A.__dict__Out[6]: mappingproxy(&#123;'__module__': '__main__', 'a': 40, '_a': 50, '_A__a': 60, '__init__': &lt;function __main__.A.__init__(self)&gt;, '_A__test': &lt;function __main__.A.__test(self)&gt;, '__dict__': &lt;attribute '__dict__' of 'A' objects&gt;, '__weakref__': &lt;attribute '__weakref__' of 'A' objects&gt;, '__doc__': None&#125;) In [7]: A._A__aOut[7]: 60 实例化对象也是通过加前单下划线类名访问变量或函数 12345In [8]: a_obj._A__bOut[8]: 70In [9]: a_obj._A__test()__test 双前导和双末尾下划线 varPython保留了有双前导和双末尾下划线的名称，用于特殊用途。 init对象构造函数 call它使得一个对象可以被调用 单下划线 _当赋值的变量无关紧要时，可以通过”_”表示其为一个临时值1234567In [26]: color,_,_,_ = ('red', 'auto', 12, 3812.4)In [28]: for key,_ in &#123;"a":"A","b":"B"&#125;.items(): ...: print(key) ...: ab “_”是大多数Python REPL中的一个特殊变量，它表示由解释器评估的最近一个表达式的结果。1234567895+6Out[2]: 11_Out[3]: 11list()Out[4]: []_.append(1)_Out[6]: [1] ALL看Django源码的时候会看到在代码最前面会有一个 all，其实all对象是装有字符串的列表对象，他会覆盖 from import 的默认行为，可以把下划线开头的变量的字符串形式加入到all中，这样 import 也能看到这些变量。 参考 https://dbader.org/blog/meaning-of-underscores-in-python https://zhuanlan.zhihu.com/p/36173202 https://foofish.net/python_xiahuaxian.html]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[组合两个表]]></title>
    <url>%2F2019%2F04%2F10%2F1%2F</url>
    <content type="text"><![CDATA[通过连接查询取得数据库字段 Person表 列名 类型 PersonId int （主） FirstName varchar LastName varchar Address表 列名 类型 AddressId int（主） PersonId int （外） City varchar State varchar 题目 编写一个SQL查询，满足条件：无论person是否有地址信息，都需要基于上述两表提供person的以下信息： FirstName, LastName, City, State 本题目考察的知识点有两点，关联查询，on where区别 数据库在通过连接两张或多张表来返回记录时，都会生成一张中间的临时表，然后再将这张临时表返回给用户。 本题中无论person是否有地址信息，都会返回person信息，所以选用左外连接（left join）。 on 和where区别： 1.on条件是在生成临时表时使用的条件，它不管on中的条件是否为真，都会返回左边表中的记录。 2.where条件是在临时表生成好后，再对临时表进行过滤的条件。这时已经没有left join的含义（必须返回左边表的记录）了，条件不为真的就全部过滤掉。 3.因为inner join取并集，因此on where 没区别 答案123SELECT p.FirstName, p.LastName, a.City, a.State FROM Person p LEFT JOIN Address a ON p.PersonId = a.personId; on与where区别]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>leetcode</tag>
        <tag>join</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[decorator]]></title>
    <url>%2F2019%2F03%2F12%2F1%2F</url>
    <content type="text"><![CDATA[python中，函数可以像变量一样当作参数传递给另一个函数， 装饰器则沿用了这一特性，在不改变既有代码的前提下，增加功能。 描述装饰器本质上是一个 Python 函数或类，返回值也是一个函数/类对象。 原则： 不修改被修饰函数的源代码 不修改被修饰函数的调用方式 装饰器 = 高阶函数+函数嵌套+闭包 高阶函数函数接受参数为函数名或函数返回值是函数名 1234def add(x, y, f): return f(x) + f(y) add(-5,6,abs) # 传入两个变量和一个绝对值函数名 return 11 函数嵌套函数内部定义函数 1234567def outer(): def inner(): print("inner") print("outer") inner() outer() 闭包闭包，顾名思义，就是一个封闭的包裹，里面包裹着自由变量，就像在类里面定义的属性值一样，自由变量的可见范围随同包裹，哪里可以访问到这个包裹，哪里就可以访问到这个自由变量。 通常函数被调用后，局部变量变失去作用域，闭包可以避免使用全局变量，使得变量脱离了函数本身的作用范围，局部变量依旧可以被访问得到 12345678910def adder(x): def wrapper(y): return x + y return wrapper adder5 = adder(5)# 输出 15adder5(10)# 输出 11adder5(6) 简单装饰器函数进入和退出时 ，被称为一个横切面，这种编程方式被称为面向切面的编程。在java Spring框架aop就是面向切面编程 12345678910111213141516import timedef timmer(func): def wrapper(*args,**kwargs): start_time=time.time() res=func(*args,**kwargs) stop_time = time.time() print(f'运行时长&#123;stop_time-start_time&#125;') return res return wrapperdef IO_operation(): time.sleep(4)IO_operation = timmer(IO_operation) # 因为装饰器 timmer(IO_operation) 返回是函数对象 wrapper，这条语句相当于 IO_operation = wrapperIO_operation() @ 语法糖 简化调用12345@timmerdef IO_operation(): time.sleep(4)IO_operation() 带参数的装饰器装饰器的语法允许我们在调用时，提供其它参数,它实际上是对原有装饰器的一个函数封装，并返回一个装饰器. 1234567891011121314151617181920212223def timmer(level): def decorator(func): def wrapper(*args,**kwargs): start_time=time.time() res=func(*args,**kwargs) stop_time = time.time() if level=1: print(f'1:&#123;stop_time-start_time&#125;') if level=2: print(f'2:&#123;stop_time-start_time&#125;') return res return wrapper return decorator def IO_operation(): time.sleep(4) @timmer(1)def IO_operation(): time.sleep(4)IO_operation() 类装饰器类装饰器主要依靠类的call方法,当一个类如果实现了call方法，那么该类的实例对象的行为就是一个函数，是一个可以被调用（callable)的对象。123456789class Add: def __init__(self, n): self.n = n def __call__(self, x): return self.n + x&gt;&gt;&gt; add = Add(1)&gt;&gt;&gt; add(4)&gt;&gt;&gt; 5 确定对象是否为可调用对象通过内置函数callable来判断123456&gt;&gt;&gt; callable(foo)True&gt;&gt;&gt; callable(1)False&gt;&gt;&gt; callable(int)True 类装饰器实现123456789101112131415class Foo(object): def __init__(self, func): self._func = func def __call__(self): print ('class decorator runing') self._func() print ('class decorator ending')@Foodef bar(): print ('bar')bar() functools.wraps保留函数元信息使用装饰器极大地复用了代码，但是他有一个缺点就是原函数的元信息被内部函数得元信息替代，python内部提供functools.wraps装饰器，它能把原函数的元信息拷贝到装饰器里面的 func 函数中，这使得装饰器里面的 func 函数也有和原函数 foo 一样的元信息了。 12345678910111213from functools import wrapsdef logged(func): @wraps(func) def with_logging(*args, **kwargs): print func.__name__ # 输出 'f' print func.__doc__ # 输出 'does some math' return func(*args, **kwargs) return with_logging@loggeddef f(x): """does some math""" return x + x * x 装饰器顺序一个函数还可以同时定义多个装饰器，比如：1234567@a@b@cdef f (): pass &gt;&gt;&gt;f = a(b(c(f))) #执行顺序是从里到外 参考： https://foofish.net/python-closure.html https://foofish.net/python-decorator.html https://foofish.net/function-is-first-class-object.html]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>设计模式</tag>
        <tag>装饰器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[uWSGI_nginx_Django]]></title>
    <url>%2F2019%2F03%2F11%2F1%2F</url>
    <content type="text"><![CDATA[通过uWSGI+nginx+Django搭建生产web服务 系统环境参数操作系统：Ubuntu 18.04.1 LTS python版本：3.6.7 Django版本：2.1.7 nginx版本：nginx/1.14.0 (Ubuntu) 服务搭建本文使用虚拟环境保证环境纯净 12345virtualenv -p /usr/bin/python3 uwsgi-tutorialcd uwsgi-tutorialsource bin/activatepip install uwsgipip install django 退出当前虚拟环境deactivate uWSGI介绍一个web服务器面对的是外部世界。它能直接从文件系统提供文件 (HTML, 图像， CSS等等)。然而，它无法 直接与Django应用通信；它需要借助一些工具的帮助，这些东西会运行运用，接收来自web客户端（例如浏览器）的请求，然后返回响应。 一个Web服务器网关接口（Web Server Gateway Interface） - WSGI - 就是干这活的。 WSGI 是一种Python标准。 uWSGI是一种WSGI实现。在这个教程中，我们将设置uWSGI，让它创建一个Unix socket，并且通过WSGI协议提供响应到web服务器。 基础测试当前目录 /home/darren 创建test.py vim test.py 1234def application(env, start_response): start_response('200 OK', [('Content-Type','text/html')]) return [b"Hello World"] # python3 #return ["Hello World"] # python2 uwsgi --http :8000 --wsgi-file test.py http :8000: 使用http协议，端口8000 wsgi-file test.py: 加载指定的文件，test.py 浏览器访问http://192.168.98.129:8000/ 显示Hello World 组件栈1the web client &lt;-&gt; uWSGI &lt;-&gt; Python 运行Djangodjango基础运行方式 1234567django-admin.py startproject mysitecd mysitevim mysite/setting.py# 设置允许访问ALLOWED_HOSTS = [&apos;*&apos;]python manage.py runserver uwsgi运行Django 12cd mysiteuwsgi --http :8000 --module mysite.wsgi 组件栈1the web client &lt;-&gt; uWSGI &lt;-&gt; Django nginx浏览器一般不会直接和uWSGI打交道，它通过连接web服务器，利用uWSGI创建Unix socket，和Django应用建立关系。 1234567# installsudo apt-get install nginx# basic operationsudo /etc/init.d/nginx startsudo /etc/init.d/nginx stopsudo /etc/init.d/nginx restart 为站点配置nginx需要 uwsgi_params文件12345678910111213141516uwsgi_param QUERY_STRING $query_string;uwsgi_param REQUEST_METHOD $request_method;uwsgi_param CONTENT_TYPE $content_type;uwsgi_param CONTENT_LENGTH $content_length;uwsgi_param REQUEST_URI $request_uri;uwsgi_param PATH_INFO $document_uri;uwsgi_param DOCUMENT_ROOT $document_root;uwsgi_param SERVER_PROTOCOL $server_protocol;uwsgi_param REQUEST_SCHEME $scheme;uwsgi_param HTTPS $https if_not_empty;uwsgi_param REMOTE_ADDR $remote_addr;uwsgi_param REMOTE_PORT $remote_port;uwsgi_param SERVER_PORT $server_port;uwsgi_param SERVER_NAME $server_name; vim mysite_nginx.conf 通过配置告诉nginx提供来自文件系统的媒体和静态文件，以及处理那些需要Django干预的请求123456789101112131415161718192021222324252627282930313233# mysite_nginx.conf# the upstream component nginx needs to connect toupstream django &#123; # server unix:///home/darren/mysite/mysite.sock; # for a file socket server 127.0.0.1:8001; # for a web port socket (we&apos;ll use this first)&#125;# configuration of the serverserver &#123; # the port your site will be served on listen 8000; # the domain name it will serve for server_name .example.com; # substitute your machine&apos;s IP address or FQDN charset utf-8; # max upload size client_max_body_size 75M; # adjust to taste # Django media location /media &#123; alias /home/darren/mysite/media; # your Django project&apos;s media files - amend as required &#125; location /static &#123; alias /home/darren/mysite/static; # your Django project&apos;s static files - amend as required &#125; # Finally, send all non-media requests to the Django server. location / &#123; uwsgi_pass django; include /home/darren/mysite/uwsgi_params; # the uwsgi_params file you installed &#125;&#125; 设置链接让nginx可以读取sudo ln -s /home/darren/mysite/mysite_nginx.conf /etc/nginx/sites-enabled/ 部署静态文件12vim mysite.setting.pySTATIC_ROOT = os.path.join(BASE_DIR, &quot;static/&quot;) 运行 python manage.py collectstatic 重启nginx sudo /etc/init.d/nginx restart uwsgi + nginx +Django客户端从发送一个 HTTP 请求到 Django 处理请求，分别经过了 web服务器层，WSGI层，web框架层. Web服务器层 对于传统的客户端服务器架构，其请求的处理过程是，客户端向服务器发送请求，服务器接收请求并处理请求，然后给客户端返回响应。 WSGI层 定义了 web服务器和 web应用之间的接口规范。uWSGI作为中间层存在 Web框架层 处理实现业务的逻辑 TCP端口socket使用uwsgi协议，端口为8001,nginx配置中暴露8000端口与uWSGI通信uwsgi --socket :8001 --module mysite.wsgi 使用Unix socket编辑 mysite_nginx.conf: 12server unix:///home/darren/mysite/mysite.sock; # for a file socket# server 127.0.0.1:8001; # for a web port socket (we&apos;ll use this first) 运行uwsgi --socket mysite.sock --module mysite.wsgi 可能会出现启动失败，可以 tail -f /var/log/nginx/error.log动态查看错误信息。 本人在操作的时候出现的错误12019/03/11 07:18:51 [crit] 51025#51025: *55 connect() to unix:///home/darren/mysite/mysite.sock failed (13: Permission denied) while connecting to upstream, client: 192.168.98.1, server: example.com, request: "GET / HTTP/1.1", upstream: "uwsgi://unix:///home/darren/mysite/mysite.sock:", host: "192.168.98.129:8000" 更改套接字文件的权限，以便www-data可以写入它。 uwsgi --socket mysite.sock --module mysite.wsgi --chmod-socket=666 组件栈 the web client &lt;-&gt; the web server &lt;-&gt; the socket &lt;-&gt; uWSGI &lt;-&gt; Django 配置uWSGI以允许.ini文件命令行启动参数过多可以通过启动配置文件的方式 创建vim mysite_uwsgi.ini 123456789101112131415161718192021# mysite_uwsgi.ini file[uwsgi]# Django-related settings# the base directory (full path)chdir = /home/darren/mysite# Django's wsgi filemodule = mysite.wsgi# the virtualenv (full path)home = /home/darren/uwsgi-tutorial# process-related settings# mastermaster = true# maximum number of worker processesprocesses = 2# the socket (use the full path to be safesocket = /home/darren/mysite/mysite.sock# ... with appropriate permissions - may be neededchmod-socket = 666# clear environment on exitvacuum = true 运行配置文件启动服务 uwsgi --ini mysite_uwsgi.ini 本人在操作时出现错误，如下： 1no request plugin is loaded, you will not be able to manage requests. you may need to install the package for your language of choice, or simply load it with --plugin. 在stackoverflow找到解决方案: 1apt-get install uwsgi-plugin-python3 在mysite_uwsgi.ini添加1plugins = python3 探讨Django 自己就可以搭建web服务,为什么用uWsgi?Django自带wsgi服务器，但是性能不好，单进程。uWSGI支持的并发量更高方便管理多进程，发挥多核的优势提升性能。 uWSGI可以作为Django web服务端，为什么还需要用到nginx?uWSGI，是一个程序，充当Web服务器或中间件。如果架构是Nginx+uWSGI+APP，uWSGI是一个中间件.如果架构是uWSGI+APP，uWSGI是一个服务器。 nginx改进了静态资源的处理，可以显着减少服务器和网络负载 nginx也可以缓存一些动态内容 nginx 作为专业服务器，暴露在公网相对比较安全 nginx可以进行多台机器的负载均衡 nginx可以更好地配合CDN 参考： uwsgi Nginx+uWSGI+Django原理 stackExchage 如何理解Nginx, WSGI, Flask之间的关系]]></content>
      <categories>
        <category>django</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>django</tag>
        <tag>uWSGI</tag>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker基础]]></title>
    <url>%2F2019%2F03%2F05%2Fdocker-basis%2F</url>
    <content type="text"><![CDATA[docker的出现为环境配置，项目部署提供了空前便捷。相较于虚拟机的臃肿，运行缓慢，docker既轻量有快捷，成本开销都更小 docker install下载适配当前liunx版本的docker sudo wget -q0- https://get.docker.com | sh docker需要使用root用户运行，可以通过将用户添加到”docker” group,注销并重新登录使其生效 sudo usermod -aG docker [USER] 命令image 操作命令查找image(支持通配符) docker search [IMAGE] 获取 image docker pull [IMAGE] 查看 image(支持通配符) docker images 删除 image docker rmi [IMAGE] 查看image 历史（创建过程） docker history ubuntu:latest 查看详细信息 docker inspect ubuntu:latest 添加镜像标签（指向同一个镜像文件，只是别名不同） docker tag ubuntu:latest myubuntu:latest 创建 image docker build [IMAGE] container 操作命令启动容器(通过本地镜像创建容器并启动，本地没有远端拉取) docker run [IMAGE] 终端启动容器 docker run -it -p 80:80 [IMAGE] \bin\bash 守护进程启动容器 docker run --name webserver -d -p 80:80 [IMAGE] -d 守护进程后台启动 –name 容器名 -p 端口映射 容器关闭时清理容器的匿名data volumes docker run –rm -it [IMAGE] 通过交互终端打开容器（一般用于打开守护进程在后端启动的容器） docker exec -it webserver bash 查看容器修改（会有大量无关内容） docker diff webserver 清理所有处于终止状态的容器 docker container prune 查看当前运行的 container(options -a 所有容)docker ps 关闭容器(start 启动 restart 重启) docker stop [IMAGE] 删除容器(需要关闭) docker rm [IMAGE] 关于镜像删除删除行为分为两类，一类是 Untagged，另一类是 Deleted。当一个镜像有多个标签时,删除只是删除该镜像的的某一个标签(Untagged),当只有一个标签就会彻底删除(Deleted)。 当有该镜像创建的容器存在时，镜像无法删除，需要删除容器，docker rmi -f [IMAGE] 强制删除镜像 （不推荐） 关于–rm操作在Docker容器退出时，默认容器内部的文件系统仍然被保留，以方便调试并保留用户数据。 但是，对于foreground容器，由于其只是在开发调试过程中短期运行，其用户数据并无保留的必要，因而可以在容器启动时设置–rm选项，这样在容器退出时就能够自动清理容器内部的文件系统。示例如下： docker run –rm [IMAGE] 显然，–rm选项不能与-d同时使用，即只能自动清理foreground容器，不能自动清理detached容器 注意，–rm选项也会清理容器的匿名data volumes，容器也会一并删除。 创建镜像 从已经创建的容器中更新镜像，commit提交这个镜像 基于本地模板导入 使用 Dockerfile指令来创建一个新的镜像 基于已有容器创建docker commit [选项] &lt;容器ID或容器名&gt; [&lt;仓库名&gt;[:&lt;标签&gt;]] docker commit --author &#39;darren&#39; --message &#39;修改首页&#39; dad9a2486a8b nginx:darr 黑盒操作，虽然可以diff命令查看，但维护麻烦，会使得镜像臃肿。 导入创建镜像导出 docker save [IMAGE ID] &gt; save.tar 镜像的导入 docker load &lt; save.tar 容器导出(导出容器的快照) docker export [IMAGE ID] &gt; export.tar 容器导入 cat export.tar | docker import - [IMAGE NAME:TAG] 镜像导入和容器导入的区别： 容器导入 是将当前容器 变成一个新的镜像,容器快照文件将丢弃所有的历史记录和元数据信息（即仅保存容器当时的快照状态） 镜像导入 是复制的过程,体积更大 save 和 export区别： save 保存镜像所有的信息-包含历史 export 只导出当前的信息 Dockerfile在 Dockerfile 文件所在目录执行：docker build -t [IMAGE TAG] . 镜像构建上下文（Context） 当构建的时候，用户会指定构建镜像上下文的路径，docker build 命令得知这个路径后，会将路径下的所有内容打包，然后上传给 Docker 引擎。这样 Docker引擎收到这个上下文包后，展开就会获得构建镜像所需的一切文件。所需文件需存放在dockerfile同级目录。 指令：FROMRUNCOPYADDCMDENTRYPOINTENVARGVOLUMEEXPOSEWORKERDIRUSERHEALTHCHECKONBUILD ADD COPY 区别： ADD可以将远端的文件复制进容器 ADD可以在复制压缩包进容器时会自动解压 CMD ENTRYPOINT HEALTHCHECK 只能有一个，输入多个最后一个生效 CMD ENTRYPOINT 区别 run container 后面带的参数 会将 CMD 指令覆盖，但是不能覆盖ENTRYPOINT指令，会变成ENTRYPOINT的参数添加。docker run -it --entrypoint=[指令] [IMAGE] 就可以覆盖ENTRYPOINT指令 对于容器而言，其启动程序就是容器应用进程，容器就是为了主进程而存在的，主进程退出，容器就失去了存在的意义，从而退出，其它辅助进程不是它需要关心的东西。守护进程一般在系统启动时开始运行，除非强行终止，否则直到系统关机都保持运行,因此会以守护进程的方式在后端启动服务。 Docker 容器通过虚拟网桥进行连接]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[__dict__]]></title>
    <url>%2F2019%2F02%2F24%2Fdict%2F</url>
    <content type="text"><![CDATA[Python下一切皆对象，每个对象都有多个属性(attribute)，Python对属性有一套统一的管理方案。dict是用来存储对象属性的一个字典，其键为属性名，值为属性的值。 dict访问对象属性dict属性是一个字典(dict)，它包含了该对象所有的属性。12345678910111213141516class Parent(object): a = 0 b = 1 def __init__(self): self.a = 2 self.b = 3 def p_test(self): passp = Parent()print(Parent.__dict__)print(p.__dict__) OutPut:12&#123;'__module__': '__main__', 'a': 0, 'b': 1, '__init__': &lt;function Parent.__init__ at 0x0000021251F037B8&gt;, 'p_test': &lt;function Parent.p_test at 0x000002125218DAE8&gt;, '__dict__': &lt;attribute '__dict__' of 'Parent' objects&gt;, '__weakref__': &lt;attribute '__weakref__' of 'Parent' objects&gt;, '__doc__': None&#125;&#123;'a': 2, 'b': 3&#125; python中的内置的数据类型（如int, list, dict）不存在dict属性 继承状态下dict属性子类有自己的dict,父类也有自己的dict,子类的类变量和函数放在子类的dict中，父类的放在父类dict中。 父子类对象有自己的dict属性， 存储self.xx 信息，子类不重写父类实例化变量，父子类对象dict一致，子类重写父类实例化变量，子类对象dict会发生改变 123456789101112131415161718192021222324252627282930313233class Parent(object): a = 0 b = 1 def __init__(self): self.a = 2 self.b = 3 def p_test(self): passclass Child(Parent): # a = 4 # b = 5 def __init__(self): super(Child, self).__init__() # self.a = 6 # self.b = 7 def c_test(self): pass def p_test(self): passp = Parent()c = Child()print(Parent.__dict__)print(Child.__dict__)print(p.__dict__)print(c.__dict__) 1234&#123;'__module__': '__main__', 'a': 0, 'b': 1, '__init__': &lt;function Parent.__init__ at 0x000001FC798437B8&gt;, 'p_test': &lt;function Parent.p_test at 0x000001FC79ACDAE8&gt;, '__dict__': &lt;attribute '__dict__' of 'Parent' objects&gt;, '__weakref__': &lt;attribute '__weakref__' of 'Parent' objects&gt;, '__doc__': None&#125;&#123;'__module__': '__main__', '__init__': &lt;function Child.__init__ at 0x000001FC79ACDB70&gt;, 'c_test': &lt;function Child.c_test at 0x000001FC79ACDBF8&gt;, 'p_test': &lt;function Child.p_test at 0x000001FC79ACDC80&gt;, '__doc__': None&#125;&#123;'a': 2, 'b': 3&#125;&#123;'a': 2, 'b': 3&#125; dict操作私有变量java中可以通过反射机制操作到对象的私有属性,python中可以通过dict操作私有变量 __xx：双前置下划线，私有化属性或方法，无法在外部直接访问（名字重整所以访问不到）123456789101112131415class test(object): def __init__(self): self.num = 10 self.__num = 30 def __str__(self): return f"num = &#123;self.num&#125;,__num = &#123;self.__num&#125;"t = test()print(t.num) # 10# print(t.__num) # AttributeError: 'test' object has no attribute '__num'print(t.__dict__['_test__num'])t.__dict__['num'] = 50t.__dict__['_test__num'] = 100print(t) OutPut:1231030num = 50,__num = 100]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进程间通信]]></title>
    <url>%2F2019%2F02%2F24%2F1%2F</url>
    <content type="text"><![CDATA[多进程之间是如何进行通信的呢？ 概述进程进程是具有一定独立功能的程序关于某个数据集合上的一次运行活动,进程是系统进行资源分配和调度的一个独立单位. 线程线程是进程的一个实体,是CPU调度和分派的基本单位,它是比进程更小的能独立运行的基本单位.线程自己基本上不拥有系统资源,只拥有一点在运行中必不可少的资源(如程序计数器,一组寄存器和栈),但是它可与同属一个进程的其他的线程共享进程所拥有的全部资源. 区别进程和线程的主要差别在于它们是不同的操作系统资源管理方式。进程有独立的地址空间，一个进程崩溃后，在保护模式下不会对其它进程产生影响，而线程只是一个进程中的不同执行路径。线程有自己的堆栈和局部变量，但线程之间没有单独的地址空间，一个线程死掉就等于整个进程死掉，所以多进程的程序要比多线程的程序健壮，但在进程切换时，耗费资源较大，效率要差一些。 多进程通信陈述每个进程都有独立的地址空间，设置全局变量并不能在多进程中共享 123456789101112131415161718import timefrom multiprocessing import Processdef producer(a): a +=1 time.sleep(2)def consumer(a): time.sleep(2) print(a)if __name__ =="__main__": a = 1 my_producer = Process(target=producer,args=(a,)) my_consumer = Process(target=consumer,args=(a,)) my_producer.start() my_consumer.start() my_producer.join() my_consumer.join() OutPut: 1 multiprocessing.Queue()传统的Queue并不能满足多进程的使用 123456789101112131415161718import timefrom multiprocessing import Process,Queuedef producer(queue): queue.put("a") time.sleep(2)def consumer(queue): time.sleep(2) print(queue.get())if __name__ =="__main__": queue = Queue(10) my_producer = Process(target=producer,args=(queue,)) my_consumer = Process(target=consumer,args=(queue,)) my_producer.start() my_consumer.start() my_producer.join() my_consumer.join() OutPut: a multiprocessing.Pipe()Pipe（）函数返回一对由管道连接的连接对象，只适用于两个进程，默认情况下是双工（双向）。 Pipe的性能高于queue 123456789101112131415161718192021222324import timefrom multiprocessing import Pipe, Processdef producer(pipe): pipe.send("a") time.sleep(2)def consumer(pipe): time.sleep(2) print(pipe.recv())if __name__ == "__main__": recevie_pipe, send_pipe = Pipe() my_producer = Process(target=producer, args=(send_pipe,)) my_consumer = Process(target=consumer, args=(recevie_pipe,)) my_producer.start() my_consumer.start() my_producer.join() my_consumer.join() OutPut: a multiprocessing.Manager()通过Manager进行共享内存操作12345678910111213def add_data(p_dict,key,value): p_dict[key] = valueif __name__ =="__main__": process_dict = Manager().dict() first_process = Process(target=add_data,args=(process_dict,"Darr_en1",22)) second_process = Process(target=add_data,args=(process_dict,"Darr_en2",44)) first_process.start() second_process.start() first_process.join() second_process.join() print(process_dict) OutPut: {&#39;Darr_en1&#39;: 22, &#39;Darr_en2&#39;: 44}]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>process</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[详解redis install]]></title>
    <url>%2F2018%2F12%2F29%2F1%2F</url>
    <content type="text"><![CDATA[redis作为使用内存存储的NoSQL,数据存储相较于memcached有更加丰富的数据结构支持。还附有发布订阅，主从复制等功能。让我们来尝试怎么安装它吧。 linux install redis安装步骤本人使用的是ubuntu_16_server,首先需要在redis官网下载压缩包,当然网络环境允许的情况下可以通过linux命令直接下载，以下为整个俺喜欢过程：12345678# 下载压缩包$ wget http://download.redis.io/releases/redis-5.0.3.tar.gz # 解压缩$ tar xzf redis-5.0.3.tar.gz # 进入redis目录 $ cd redis-5.0.3 # 编译Redis$ make 问题大多数时候make都会出错，需要下载gcc工具123$ sudo apt-get clean$ sudo apt-get update $ sudo apt-get build-dep gcc 远程访问很多时候程序和redis不在一台服务器上，就需要访问远程Redis服务，Redis默认也只允许本机访，因此需要需改Redis的配置文件 /etc/redis/redis.conf 解决办法： 注释掉bind 127.0.0.1可以使所有的ip访问redis 修改 protected-mode no 指定配置文件然后重启Redis服务：$ sudo redis-server /etc/redis/redis.conf 远程连接：$ redis-cli -h {redis_host} -p {redis_port} windows install rediswindows安装redis弊端windows安装本人是不建议的，首先官方不支持，redis在windows上的版本会比较低，其次，redis在将数据库持久化到硬盘的时候，需要用到fork系统调用，而Windows并不支持这个调用。在缺少fork调用情况，redis在持久化操作期间就只能够阻塞所有客户端，直到持久化操作完毕为止。虽然也有提供相应解决方案，但依旧不如linux上的支持完整。 安装步骤安装redis可以在windows_redis下载,最好下载.msi后缀的文件，在安装过程中会自动写入到windows的服务中，安装完成后，进入redis目录运行redis-server.exe,可能会运行不起来，进行如下操作即可：1234redis-cli.exeshutdownexitredis-server.exe]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[re(正则表达式)]]></title>
    <url>%2F2018%2F11%2F30%2Fre%2F</url>
    <content type="text"><![CDATA[正则表达式（或RE）指定一组匹配它字符串；此模块中的函数让你检查一个特定的字符串是否匹配给定的正则表达式（或给定的正则表达式是否匹配特定的字符串）。 基本语法re.match 从头匹配原型: match(pattern,string,flags=0) 参数： patter:匹配的正则表达式 string:要匹配的字符串 flags:标志位，控制正则表达式匹配方式有如下：123456re.I 使匹配对大小写不敏感re.L 做本地化识别（locale-aware）匹配re.M 多行匹配，影响 ^ 和 $re.S 使 . 匹配包括换行在内的所有字符re.U 根据Unicode字符集解析字符。这个标志影响 \w, \W, \b, \B.re.X 该标志通过给予你更灵活的格式以便你将正则表达式写得更易于理解。 功能： 从头开始匹配返回第一个匹配对象，匹配失败返回None Demo:12print(re.match('www', 'www.Darr_en1.com').group()) #wwwprint(re.match('arr', 'www.Darr_en1.com').group()) #AttributeError: 'NoneType' object has no attribute 'group' re.search 匹配包含原型: search(pattern,string,flags=0) 功能： 扫描整个字符串返回第一个匹配对象，匹配失败返回None Demo:12print(re.search('www', 'www.Darr_en1.com').group()) #wwwprint(re.search('arr', 'www.Darr_en1.com').group()) #arr re.compile 预编译原型: compile(pattern,flags=0) 功能： 把正则表达式编译成一个正则对象 在看《python核心编程》（第3版）对于编译的说法，深有体会，以下摘录： 编译正则表达式（编译还是不编译？）&emsp;&emsp;在 Core Python Programming 或者即将出版的 Core Python Language Fundamentals 的执行环境章节中，介绍了Python代码最终如何被编译成字节码，然后在解释器上执行。特别是，我们指定 eval()或者 exec（在 2.x 版本中或者在 3.x 版本的 exec()中）调用一个代码对象而不是一个字符串，性能上会有明显提升。这是由于对于前者而言，编译过程不会重复执行。换句话说，使用预编译的代码对象比直接使用字符串要快，因为解释器在执行字符串形式的代码前都必须把字符串编译成代码对象。&emsp;&emsp;同样的概念也适用于正则表达式—在模式匹配发生之前，正则表达式模式必须编译成正则表达式对象。由于正则表达式在执行过程中将进行多次比较操作，因此强烈建议使用预编译。而且，既然正则表达式的编译是必需的，那么使用预编译来提升执行性能无疑是明智之举。re.compile()能够提供此功能。&emsp;&emsp;其实模块函数会对已编译的对象进行缓存，所以不是所有使用相同正则表达式模式的 search()和 match()都需要编译。即使这样，你也节省了缓存查询时间，并且不必对于相同的字符串反复进行函数调用。在不同的 Python 版本中，缓存中已编译过的正则表达式对象的数目可能不同，而且没有文档记录。purge()函数能够用于清除这些缓存。 Demo:12re_obj = re.compile("www")print(re_obj.match('www.Darr_en1.com').group()) #www re.findall 匹配包含返回列表原型: findall(pattern,string,flags=0) 功能： 扫描整个字符串返回结果列表，匹配失败返回None Demo:12mail='&lt;Darr_en101@mail.com&gt; &lt;Darr_en102@mail.com&gt; Darr_en103@mail.com'print(re.findall(r'(\w+@m....[a-z]&#123;3&#125;)',mail)) #['Darr_en101@mail.com', 'Darr_en102@mail.com', 'Darr_en103@mail.com'] re.finditer 匹配包含返回迭代器原型: findall(pattern,string,flags=0) 功能： 扫描整个字符串返回迭代器，可防止内存过大现象 Demo:12mail='&lt;Darr_en101@mail.com&gt; &lt;Darr_en102@mail.com&gt; Darr_en103@mail.com'print(next(re.finditer(r'(\w+@m....[a-z]&#123;3&#125;)',mail)).group()) #Darr_en101@mail.com re.split 以匹配到的字符当做列表分隔符原型: split(pattern, string, maxsplit=0, flags=0) 参数： maxsplit：最大分割字符串，默认为0，表示每个匹配项都分割 Demo:123str ="Darr_en1 is so cool"print(re.split(r' +',str)) #['Darr_en1', 'is', 'so', 'cool']print(str.split(' ')) #['Darr_en1', '', '', '', '', 'is', 'so', 'cool'] re.sub 替换字符串的匹配项原型： sub(pattern, repl, string, count=0) 参数： rep1:替换后的字符串 count：替换个数,默认为0，表示每个匹配项都替换 功能： 替换字符串的匹配项，返回str Demo:123str="Darr_en1 is so cool"print(re.sub(r'\s','-',str)) #Darr_en1-is-so-coolprint(re.sub(r'\s','-',str,2)) #Darr_en1-is-so cool re.subn 替换字符串的匹配项原型： subn(pattern, repl, string, count=0) 参数： rep1:替换后的字符串 count：替换个数,默认为0，表示每个匹配项都替换 功能： 替换字符串的匹配项，返回tuple,第一个值为被替换的str，第二个值为被替换次数 Demo:123str="Darr_en1 is so cool"print(re.subn(r'\s','-',str)) #('Darr_en1-is-so-cool', 3)print(re.subn(r'\s','-',str,2)) #('Darr_en1-is-so cool', 2) group() groups() 分组功能： 通过()来进行分组，group(n)返回第n个值,groups()返回tuple Demo：12345678str="Darr_en1-is-so-cool"print(re.findall(r'(\w+)(-)',str)) #[('Darr_en1', '-'), ('is', '-'), ('so', '-')]print(re.findall(r'\w+-',str)) #['Darr_en1-', 'is-', 'so-']print(re.match(r'(\w+)(-)',str).group()) # Darr_en1-print(re.match(r'(\w+)(-)',str).group(0)) # Darr_en1-print(re.match(r'(\w+)(-)',str).group(1)) # Darr_en1print(re.match(r'(\w+)(-)',str).group(2)) # -print(re.match(r'(\w+)(-)',str).groups()) # ('Darr_en1', '-') 常用正则表达式符号单个字符12&apos;.&apos; 匹配除\n之外的任意一个字符，若指定flag DOTALL,则匹配任意字符，包括换行&apos;[]&apos; 匹配括号内的所有字符,第一个字符^为取反 预定义字符集123456&apos;\d&apos; 匹配数字[0-9]&apos;\D&apos; 匹配非数字,即[^0-9]&apos;\w&apos; 匹配单词，即[A-Za-z0-9_]&apos;\W&apos; 匹配非单词,即[^A-Za-z0-9_]&apos;\s&apos; 匹配空格、\t、\n、\r , 即[&lt;空格&gt;\f\n\r\t],re.search(&quot;\s+&quot;,&quot;ab\tc1\n3&quot;).group() 结果 &apos;\t&apos;&apos;\S&apos; 匹配非空格、\t、\n、\r , 即[^&lt;空格&gt;\f\n\r\t] 锚字符（边界字符）123456&apos;^&apos; 匹配字符开头，若指定flags MULTILINE,这种也可以匹配上(r&quot;^a&quot;,&quot;\nabc\neee&quot;,flags=re.MULTILINE)&apos;\A&apos; 匹配字符开头，同$区别是，只匹配整个字符串开头，在re.M下匹配第一行开头&apos;$&apos; 匹配字符结尾，或e.search(&quot;foo$&quot;,&quot;bfoo\nsdfsf&quot;,flags=re.MULTILINE).group()也可以&apos;\Z&apos; 匹配字符结尾，同$区别是，只匹配整个字符串结尾，在re.M下匹配最后一行结尾&apos;\b&apos; 匹配单词边界，即匹配\W和\w之间的位置&apos;\B&apos; 匹配非单词边界 量数词(匹配多个字符)123456789&apos;*&apos; 匹配*号前的字符0次或多次，re.findall(&quot;ab*&quot;,&quot;cabb3abcbbac&quot;) 结果为[&apos;abb&apos;, &apos;ab&apos;, &apos;a&apos;]&apos;+&apos; 匹配前一个字符1次或多次，re.findall(&quot;ab+&quot;,&quot;ab+cd+abb+bba&quot;) 结果[&apos;ab&apos;, &apos;abb&apos;]&apos;?&apos; 匹配前一个字符1次或0次&apos;&#123;m&#125;&apos; 匹配前一个字符m次&apos;&#123;n,m&#125;&apos; 匹配前一个字符n到m次，re.findall(&quot;ab&#123;1,3&#125;&quot;,&quot;abb abc abbcbbb&quot;) 结果&apos;abb&apos;, &apos;ab&apos;, &apos;abb&apos;]&apos;&#123;m,&#125;&apos; 匹配前一外字符至少 m次 至多无限次；&apos;&#123;,n&#125;&apos; 匹配前一个字符 0 到 n次&apos;|&apos; 匹配|左或|右的字符，re.search(&quot;abc|ABC&quot;,&quot;ABCBabcCD&quot;).group() 结果&apos;ABC&apos;&apos;(...)&apos; 分组匹配，re.search(&quot;(abc)&#123;2&#125;a(123|456)c&quot;, &quot;abcabca456c&quot;).group() 结果 abcabca456c 贪婪匹配 非贪婪匹配123456789101112131415贪婪模式在整个表达式匹配成功的前提下，尽可能多的匹配。非贪婪模式在整个表达式匹配成功的前提下，尽可能少的匹配。属于贪婪模式的量词，也叫做匹配优先量词，包括：“&#123;m,n&#125;”、“&#123;m,&#125;”、“?”、“*”和“+”。属于非贪婪模式的量词，也叫做忽略优先量词，包括：“&#123;m,n&#125;?”、“&#123;m,&#125;?”、“??”、“*?”和“+?”。举例：源字符串：aa&lt;div&gt;test1&lt;/div&gt;bb&lt;div&gt;test2&lt;/div&gt;cc正则表达式一：&lt;div&gt;.*&lt;/div&gt;匹配结果一：&lt;div&gt;test1&lt;/div&gt;bb&lt;div&gt;test2&lt;/div&gt;正则表达式二：&lt;div&gt;.*?&lt;/div&gt;匹配结果二：&lt;div&gt;test1&lt;/div&gt;,&lt;div&gt;test2&lt;/div&gt;]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>re</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo blog高级配置]]></title>
    <url>%2F2018%2F11%2F28%2Fhexo_advanced_configuration%2F</url>
    <content type="text"><![CDATA[通过hexo创建完简单blog，是不是还觉得缺点啥呢？那就让我们进入高级配置把。 本blog使用主题为NexT6.5.0会和iissnan的版本有些许不一样。 如果不会搭建blog,可以参考uchuhimo老哥的文章如何使用 Hexo 和 GitHub Pages 搭建这个博客，比较详细，我就不重复造轮子了。 文章代码样式修改 行内代码修改themes\next-reloaded\source\css\_custom\custom.styl文件： 12345678code &#123; color: #c7254e; background: #f9f2f4; border: 1px solid #d6d6d6; padding:1px 4px; word-break: break-all; border-radius:4px;&#125; 区块代码修改themes\next-reloaded\config.yml文件： 12# 样式种类： normal | night | night eighties | night blue | night brighthighlight_theme: night 文章结束语设置 添加templatethemes\next-reloaded\layout\_macro中新建 passage-end-tag.swig,内容如下： 1234567&lt;div&gt; &#123;% if not is_index %&#125; &lt;div style="text-align:center;color: #ccc;font-size:14px;"&gt; -------------本文结束 感谢您的阅读------------- &lt;/div&gt; &#123;% endif %&#125;&lt;/div&gt; 导入template在themes\next-reloaded\layout\_macro\post.swig文件中,添加 12345678&lt;div&gt; &#123;% if not is_index %&#125; &#123;% include 'passage-end-tag.swig' %&#125; &#123;% endif %&#125;&lt;/div&gt;&#123;#####################&#125;&#123;### END POST BODY ###&#125;&#123;#####################&#125; 配置在themes\next-reloaded\_config.yml添加： 12passage_end_tag: enabled: true 文章底部标签修改修改themes\next-reloaded\layout\_macro\post.swig,searchrel=&quot;tag&quot;&gt;#,将 #更改为&lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt; 自定义文章的默认头部信息创建新的页面hexo new &quot;name&quot;查看source\_posts\name.md：12345---title: namedate: 2018-11-28 10:53:42 tags: --- 添加scaffolds/post.md：12345678910---title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;tags: #标签categories: #分类copyright: true #版权声明permalink: 01 #文章链接top: 0 #置顶优先级password: #密码保护--- 搜索功能 安装 1npm install hexo-generator-searchdb --save 配置 修改themes\next-reloaded\_config.yml: 1234local_search: enable: true trigger: auto top_n_per_article: 1 在_config.yml中，添加： 12345search: path: search.xml field: post format: html limit: 10000 连接样式美化添加themes\next-reloaded\source\css\_custom\custom.styl:12345678910.post-body a&#123; color: #0593d3; border-bottom: none; border-bottom: 1px solid #0593d3; &amp;:hover &#123; color: #fc6423; border-bottom: none; border-bottom: 1px solid #fc6423; &#125;&#125; 文章添加阴影效果添加themes\next-reloaded\source\css\_custom\custom.styl:1234567.post &#123; margin-top: 60px; margin-bottom: 60px; padding: 25px; -webkit-box-shadow: 0 0 5px rgba(202, 203, 203, .5); -moz-box-shadow: 0 0 5px rgba(202, 203, 204, .5); &#125; 添加动态背景打开\themes\next-reloaded\layout\_layout.swig，在&lt; /body&gt;前添加代码：123&#123;% if theme.canvas_nest %&#125;&lt;script type="text/javascript" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"&gt;&lt;/script&gt;&#123;% endif %&#125; 添加themes\next-reloaded\_config.yml:123456# --------------------------------------------------------------# background settings# --------------------------------------------------------------# add canvas-nest effect# see detail from https://github.com/hustcc/canvas-nest.jscanvas_nest: true 配置项说明 color ：线条颜色, 默认: &apos;0,0,0&apos;；三个数字分别为(R,G,B) opacity: 线条透明度（0~1）, 默认: 0.5 count: 线条的总数量, 默认: 150 zIndex: 背景的z-index属性，css属性用于控制所在层的位置, 默认: -1 添加百度分享功能添加themes\next-reloaded\_config.yml:123baidushare: type: slide baidushare: true 小萌物 install推荐使用cnpmnpm install --save hexo-helper-live2d 选择小萌物可以查看hexo-helper-live2d12345678910111213141516171819202122live2d-widget-model-chitoselive2d-widget-model-epsilon2_1live2d-widget-model-gflive2d-widget-model-haru/01 (use npm install --save live2d-widget-model-haru)live2d-widget-model-haru/02 (use npm install --save live2d-widget-model-haru)live2d-widget-model-harutolive2d-widget-model-hibikilive2d-widget-model-hijikilive2d-widget-model-izumilive2d-widget-model-koharulive2d-widget-model-mikulive2d-widget-model-ni-jlive2d-widget-model-nicolive2d-widget-model-nietzschelive2d-widget-model-nipsilonlive2d-widget-model-nitolive2d-widget-model-shizukulive2d-widget-model-tororolive2d-widget-model-tsumikilive2d-widget-model-unitychanlive2d-widget-model-wankolive2d-widget-model-z16 找到自己心仪的install。 example：npm install live2d-widget-model-haruto 配置1234567891011121314151617# 宠物live2d: enable: true scriptFrom: local pluginRootPath: live2dw/ pluginJsPath: lib/ pluginModelPath: assets/ tagMode: false log: false model: use: live2d-widget-model-haruto display: position: right width: 150 height: 300 mobile: show: true 添加版权信息 添加格式在themes\next-reloaded\layout\_macro\post.swig,post-footer所在的标签下，添加以下内容： 12345678910111213141516171819&lt;footer class="post-footer"&gt; 原有内容&lt;div&gt; &#123;# 此处判断是否在索引列表中 #&#125; &#123;% if not is_index %&#125;&lt;ul class="post-copyright"&gt; &lt;li class="post-copyright-author"&gt; &lt;strong&gt;本文作者：&lt;/strong&gt;&#123;&#123; theme.author &#125;&#125; &lt;/li&gt; &lt;li class="post-copyright-link"&gt; &lt;strong&gt;本文链接：&lt;/strong&gt; &lt;a href="&#123;&#123; url_for(page.path) &#125;&#125;" title="&#123;&#123; page.title &#125;&#125;"&gt;&#123;&#123; page.path &#125;&#125;&lt;/a&gt; &lt;/li&gt; &lt;li class="post-copyright-license"&gt; &lt;strong&gt;版权： &lt;/strong&gt; 本站文章均采用 &lt;a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" rel="external nofollow" target="_blank"&gt;CC BY-NC-SA 3.0 CN&lt;/a&gt; 许可协议，请勿用于商业，转载注明出处！ &lt;/li&gt;&lt;/ul&gt;&#123;% endif %&#125;&lt;/div&gt; 显示格式添加themes\next-reloaded\source\css\_custom\custom.styl: 1234567.post-copyright &#123; margin: 1em 0 0; padding: 0.5em 1em; border-left: 3px solid #ff1700; background-color: #f9f9f9; list-style: none;&#125;]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一篇博客]]></title>
    <url>%2F2018%2F11%2F27%2FfirstBlog%2F</url>
    <content type="text"><![CDATA[作为一个程序员，怎么可以没有一个属于自己的博客网站呢？因为最近都在从事python工作，就想用Django在云服务器上搭建一个blog，查阅的时候发现可以在github上搭建blog，功能特别完善，搭建过程也非常简单（最重要的莫过于免费啦），所以最后选用hexo在github上搭建个人博客。 之前在学习或者工作过程中，对于学到的知识一般都会写到有道云笔记。今后会把一些觉得好的文章或者是新学习的知识提炼出来，分享到我的blog中。 通过博客的方式来记录自己的学习过程，看着就很有意识呢。]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>blog</tag>
      </tags>
  </entry>
</search>
